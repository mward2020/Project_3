{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9254d42",
   "metadata": {},
   "source": [
    "\n",
    "# Mental Health Chatbot Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline to train a conversational mental health assistant. \n",
    "The system integrates emotion classification using `SamLowe/roberta-base-go_emotions` and text generation using `T5`.\n",
    "It processes multiple cleaned datasets, performs training, evaluation, and finally builds a chatbot interface using Gradio or Streamlit.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load and preprocess multiple mental health-related datasets into a consistent question/answer format\n",
    "- Simulate and encode multi-label emotion annotations using `MultiLabelBinarizer`\n",
    "- Train a RoBERTa-based emotion classification model with live metric logging (accuracy, F1, precision, recall)\n",
    "- Fine-tune two separate T5 models:\n",
    "  - One for emotionally guided chatbot response generation\n",
    "  - One for direct factual Q&A answering\n",
    "- Implement emotion-aware routing logic that selects the appropriate model at inference time\n",
    "- Create a unified RoBERTa + T5 pipeline for real-time response generation\n",
    "- Build a Gradio chatbot interface using the full system\n",
    "- Evaluate all models with live logging and inference testing\n",
    "- Save all models and tokenizers in `./saved_models/`\n",
    "- Save a `.pt` metadata file pointing to model paths for easy deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fbe97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Handling\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaConfig,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorWithPadding,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Datasets and Evaluation\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    "    concatenate_datasets,\n",
    "    Features,\n",
    "    Value,\n",
    "    Sequence\n",
    ")\n",
    "import evaluate\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# External Evaluation Libraries\n",
    "import bert_score\n",
    "import rouge_score\n",
    "\n",
    "# TorchVision\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# Gradio for UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a8d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Current Device Index: 0\n",
      "Device Name: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Device setup (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check device information\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"Using CPU as GPU is not available.\")\n",
    "\n",
    "# Optional: set default tensor type to GPU-based FloatTensor if GPU is available\n",
    "# Only uncomment if required\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_dtype(torch.float32)\n",
    "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7328221",
   "metadata": {},
   "source": [
    "## Load and Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f865c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mode' column not found.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess 'ds4'\n",
    "ds4_path = './data/ds4_mental_health_chatbot_dataset_merged_modes.csv'\n",
    "\n",
    "df = pd.read_csv(ds4_path)\n",
    "\n",
    "# Drop the \"mode\" column if it exists\n",
    "if 'mode' in df.columns:\n",
    "    df.drop(columns=['mode'], inplace=True)\n",
    "    print(\"Dropped 'mode' column.\")\n",
    "else:\n",
    "    print(\"'mode' column not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5860da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified CSV saved and overwrote ./data/ds4_mental_health_chatbot_dataset_merged_modes.csv.\n"
     ]
    }
   ],
   "source": [
    "# Overwrite original CSV explicitly\n",
    "df.to_csv(ds4_path, index=False)\n",
    "print(f\"Modified CSV saved and overwrote {ds4_path}.\")\n",
    "\n",
    "# File paths\n",
    "dataset_paths = {\n",
    "    \"ds1\": \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",\n",
    "    \"ds2\": \"./data/ds2_transformed_mental_health_chatbot.csv\",\n",
    "    \"ds3\": \"./data/ds3_mental_health_faq_cleaned.csv\",\n",
    "    \"ds4\": ds4_path,\n",
    "    \"ds5\": \"./data/ds5_Mental_Health_FAQ.csv\",\n",
    "    \"ds6\": \"./data/ds6_mental_health_counseling.csv\"\n",
    "}\n",
    "\n",
    "# Enable/disable datasets\n",
    "dataset_switches = {\n",
    "    \"ds1\": False,\n",
    "    \"ds2\": True,\n",
    "    \"ds3\": False,\n",
    "    \"ds4\": False,\n",
    "    \"ds5\": False,\n",
    "    \"ds6\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1200d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def load_and_clean_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [col.lower().strip() for col in df.columns]\n",
    "\n",
    "    # Rename columns to standard format\n",
    "    if \"prompt\" in df.columns and \"response\" in df.columns:\n",
    "        df.rename(columns={\"prompt\": \"question\", \"response\": \"answer\"}, inplace=True)\n",
    "    if \"questions\" in df.columns:\n",
    "        df.rename(columns={\"questions\": \"question\"}, inplace=True)\n",
    "    if \"answers\" in df.columns:\n",
    "        df.rename(columns={\"answers\": \"answer\"}, inplace=True)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    df = df[[\"question\", \"answer\"]].dropna().reset_index(drop=True)\n",
    "    \n",
    "    return Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b0b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 'ds2' with 172 entries.\n"
     ]
    }
   ],
   "source": [
    "# Load selected datasets\n",
    "datasets_list = []\n",
    "for name, path in dataset_paths.items():\n",
    "    if dataset_switches.get(name, False):\n",
    "        dataset = load_and_clean_csv(path)\n",
    "        print(f\"Loaded dataset '{name}' with {len(dataset)} entries.\")\n",
    "        datasets_list.append(dataset)\n",
    "\n",
    "# Validate that at least one dataset is loaded\n",
    "if not datasets_list:\n",
    "    raise ValueError(\"No datasets selected. Please enable at least one dataset in 'dataset_switches'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5857d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and split dataset\n",
    "combined_dataset = concatenate_datasets(datasets_list).shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e48dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 154\n",
      "Test dataset size: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 154\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 18\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display dataset sizes\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643608e6",
   "metadata": {},
   "source": [
    "## Prepare 'labels' Column with MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6334c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c90441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_train = train_ds.to_pandas()\n",
    "df_test = test_ds.to_pandas()\n",
    "\n",
    "# Simulated emotion annotations (replace with real data if available)\n",
    "emotions_list = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "                 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "                 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "                 'relief', 'remorse', 'sadness', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43b5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample emotion annotations (train): [['annoyance', 'admiration', 'realization'], ['curiosity', 'approval'], ['annoyance', 'optimism', 'realization']]\n",
      "Sample binarized labels (train): 0    [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1    [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_train))]\n",
    "df_test[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_test))]\n",
    "\n",
    "# Encode with MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=emotions_list)\n",
    "\n",
    "df_train[\"labels\"] = list(mlb.fit_transform(df_train[\"emotions\"]))\n",
    "df_test[\"labels\"] = list(mlb.transform(df_test[\"emotions\"]))\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Sample emotion annotations (train):\", df_train[\"emotions\"].iloc[:3].tolist())\n",
    "print(\"Sample binarized labels (train):\", df_train[\"labels\"].iloc[:3])\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "test_ds = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d45158",
   "metadata": {},
   "source": [
    "## Configure RoBERTa for Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65350e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa multi-label classification model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define RoBERTa configuration explicitly for multi-label classification\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=len(emotions_list)\n",
    ")\n",
    "\n",
    "# Load the RoBERTa model with the defined configuration\n",
    "model_emo = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    config=config\n",
    ").to(device)\n",
    "\n",
    "print(\"RoBERTa multi-label classification model loaded and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b245b",
   "metadata": {},
   "source": [
    "## Train RoBERTa with Correct Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4730962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory\n",
    "model_path = \"./saved_models/emotion_classifier\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_emo = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d34f1cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained model...\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize model\n",
    "if os.path.exists(os.path.join(model_path, 'config.json')):\n",
    "    print(\"Loading previously trained model...\")\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(model_path, config=config).to(device)\n",
    "else:\n",
    "    print(\"No existing model found. Initializing new model.\")\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(emotions_list)\n",
    "    )\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        config=config\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89979c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"emotions\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(Value(\"float32\"))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae39a14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cc57a9aa264c1d9faa0d694ec5dd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c593258925d4bbfb1eaa522c00c5af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply features format\n",
    "train_ds = train_ds.cast(features)\n",
    "test_ds = test_ds.cast(features)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_emotion(example):\n",
    "    enc = tokenizer_emo(\n",
    "        example[\"question\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128  # explicitly limit for efficiency\n",
    "    )\n",
    "    enc[\"labels\"] = example[\"labels\"]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734b8021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7ffc51f9ef43938c87e8f8404dc926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a0b0cfce2d4afda96f8d1d069b82ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "train_emo = train_ds.map(tokenize_emotion, batched=False)\n",
    "test_emo = test_ds.map(tokenize_emotion, batched=False)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Data collator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_emo, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b97fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='micro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed39210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments setup\n",
    "training_args_emo = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f95ed9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_9020\\2916640474.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer_emo = Trainer(\n",
    "    model=model_emo,\n",
    "    args=training_args_emo,\n",
    "    train_dataset=train_emo,\n",
    "    eval_dataset=test_emo,\n",
    "    tokenizer=tokenizer_emo,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43fcc73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.235268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.241722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.232985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./saved_models/emotion_classifier\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_emo.train()\n",
    "\n",
    "# Save the trained model explicitly\n",
    "trainer_emo.save_model(model_path)\n",
    "tokenizer_emo.save_pretrained(model_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7782",
   "metadata": {},
   "source": [
    "## Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "281db59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model save path\n",
    "t5_response_path = \"./saved_models/t5_response_generator\"\n",
    "os.makedirs(t5_response_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b76c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty or invalid model directory detected. Removing...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Cleanup invalid or empty model directory\n",
    "if os.path.exists(t5_response_path) and not any(\n",
    "    fname.endswith((\".bin\", \".safetensors\", \".h5\", \".index\", \".msgpack\"))\n",
    "    for fname in os.listdir(t5_response_path)\n",
    "):\n",
    "    print(\"Empty or invalid model directory detected. Removing...\")\n",
    "    shutil.rmtree(t5_response_path)\n",
    "    os.makedirs(t5_response_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "383bc983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing T5 model found. Loading 't5-small'.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load or initialize T5 model explicitly on the correct device\n",
    "if os.path.exists(os.path.join(t5_response_path, \"pytorch_model.bin\")):\n",
    "    print(\"Loading existing trained T5 model...\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(t5_response_path).to(device)\n",
    "else:\n",
    "    print(\"No existing T5 model found. Loading 't5-small'.\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0416ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for T5 chat model\n",
    "def tokenize_t5_chat(example):\n",
    "    input_text = \"chat: \" + example[\"question\"]\n",
    "    target_text = example[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8a1cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for T5 chat model\n",
    "def tokenize_t5_chat(example):\n",
    "    input_text = \"chat: \" + example[\"question\"]\n",
    "    target_text = example[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer_t5(\n",
    "        target_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).input_ids\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b62d39d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc08f21b675464292ab41ad408d0328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fba8d876daa4437882a3fb9ca1c2509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process datasets\n",
    "train_chat = train_ds.map(tokenize_t5_chat, batched=False)\n",
    "test_chat = test_ds.map(tokenize_t5_chat, batched=False)\n",
    "\n",
    "# Set dataset format explicitly\n",
    "train_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7150c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets processed and ready for training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Datasets processed and ready for training!\")\n",
    "\n",
    "# Data collator for seq2seq tasks (recommended for T5)\n",
    "data_collator_seq2seq = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=model_t5_response,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecfcdbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args_chat = TrainingArguments(\n",
    "    output_dir=t5_response_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    save_safetensors=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "798ad2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_9020\\3203116786.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_chat = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Trainer setup and training\n",
    "trainer_chat = Trainer(\n",
    "    model=model_t5_response,\n",
    "    args=training_args_chat,\n",
    "    train_dataset=train_chat,\n",
    "    eval_dataset=test_chat,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator_seq2seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa85bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty or invalid model directory detected. Removing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\data\\data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.967600</td>\n",
       "      <td>2.206888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.992700</td>\n",
       "      <td>2.087212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.715300</td>\n",
       "      <td>2.044073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! T5 response-generation model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the directory exists and is valid\n",
    "if os.path.exists(t5_response_path) and not any(\n",
    "\tfname.endswith((\".bin\", \".safetensors\", \".h5\", \".index\", \".msgpack\"))\n",
    "\tfor fname in os.listdir(t5_response_path)\n",
    "):\n",
    "\tprint(\"Empty or invalid model directory detected. Removing...\")\n",
    "\tshutil.rmtree(t5_response_path)\n",
    "\tos.makedirs(t5_response_path, exist_ok=True)\n",
    "\n",
    "# Begin training\n",
    "trainer_chat.train()\n",
    "\n",
    "# Save model and tokenizer explicitly\n",
    "trainer_chat.save_model(t5_response_path)\n",
    "tokenizer_t5.save_pretrained(t5_response_path)\n",
    "\n",
    "print(\"Training complete! T5 response-generation model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcdc2d",
   "metadata": {},
   "source": [
    "## Train T5 for Q&A Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55a4ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric setup\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define model save path\n",
    "t5_qa_path = \"./saved_models/t5_qa\"\n",
    "os.makedirs(t5_qa_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85548968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained T5 Q&A model...\n"
     ]
    }
   ],
   "source": [
    "# Load existing model or initialize new\n",
    "if os.path.exists(os.path.join(t5_qa_path, \"pytorch_model.bin\")):\n",
    "    print(\"Loading previously trained T5 Q&A model...\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(t5_qa_path).to(device)\n",
    "else:\n",
    "    print(\"No existing Q&A model found. Starting from 't5-small'.\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "126b0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_t5_qa(examples):\n",
    "    input_texts = [\"question: \" + q for q in examples[\"question\"]]\n",
    "    target_texts = examples[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(\n",
    "            target_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "        ).input_ids\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec3af213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2670f6b6784e0f872d8b12785e3e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e9ae95fe934e438c7881aff63eb76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "train_qa = train_ds.map(tokenize_t5_qa, batched=True)\n",
    "test_qa = test_ds.map(tokenize_t5_qa, batched=True)\n",
    "\n",
    "train_qa.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_qa.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Data collator\n",
    "data_collator_qa = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=model_t5_qa,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c85a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9a670d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Unpack predictions if necessary\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = [list(p) if isinstance(p, (np.ndarray, torch.Tensor)) else p for p in predictions]\n",
    "\n",
    "    # Ensure labels is a list of lists\n",
    "    if isinstance(labels[0], (int, np.integer)):\n",
    "        labels = [labels]\n",
    "    else:\n",
    "        labels = [list(l) if isinstance(l, (np.ndarray, torch.Tensor)) else l for l in labels]\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    labels = [[tokenizer_t5.pad_token_id if token == -100 else token for token in label] for label in labels]\n",
    "\n",
    "    # Decode\n",
    "    decoded_preds = tokenizer_t5.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {k: v for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ba24a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args_qa = Seq2SeqTrainingArguments(\n",
    "    output_dir=t5_qa_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    seed=42,\n",
    "    predict_with_generate=True  # ðŸ‘ˆ key for Seq2Seq generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87ca64ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_9020\\2330614297.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_qa = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trainer setup\n",
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=model_t5_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=train_qa,\n",
    "    eval_dataset=test_qa,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator_qa,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a949b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.568700</td>\n",
       "      <td>1.694290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.481800</td>\n",
       "      <td>1.634292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.394100</td>\n",
       "      <td>1.625243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! T5 Q&A model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Train and save\n",
    "trainer_qa.train()\n",
    "trainer_qa.save_model(t5_qa_path)\n",
    "tokenizer_t5.save_pretrained(t5_qa_path)\n",
    "\n",
    "print(\"Training complete! T5 Q&A model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91095b45",
   "metadata": {},
   "source": [
    "## Unified Emotion-Aware Response System (RoBERTa + T5 Routing Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a3b4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and tokenizers\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"./saved_models/emotion_classifier\")\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "t5_chat_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_response_generator\")\n",
    "t5_qa_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_qa\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2601877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labels from GoEmotions dataset\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00252b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion classifier\n",
    "def detect_emotions(text, threshold=0.5):\n",
    "    emotion_model.eval()\n",
    "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "    detected = [(emotion_labels[i], p) for i, p in enumerate(probs) if p > threshold]\n",
    "    return [label for label, _ in detected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf35d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response generator with dynamic routing\n",
    "def generate_combined_response(user_input):\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    emotional_keywords = {\n",
    "        'joy', 'sadness', 'anger', 'fear', 'love', 'grief', 'remorse', 'disappointment', 'gratitude', 'caring'\n",
    "    }\n",
    "    use_chat_model = any(e in emotional_keywords for e in emotions)\n",
    "\n",
    "    if use_chat_model:\n",
    "        prefix = \"chat: \"\n",
    "        model = t5_chat_model\n",
    "    else:\n",
    "        prefix = \"question: \"\n",
    "        model = t5_qa_model\n",
    "\n",
    "    context = f\"{' '.join(emotions)}: {user_input}\" if emotions else user_input\n",
    "    input_text = prefix + context\n",
    "\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "    response = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"Detected Emotions\": emotions,\n",
    "        \"Model Used\": \"Chat Model\" if use_chat_model else \"QA Model\",\n",
    "        \"Response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e1b65c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Detected Emotions': [], 'Model Used': 'QA Model', 'Response': ''}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it\n",
    "generate_combined_response(\"I feel really hopeless and angry all the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b38b07",
   "metadata": {},
   "source": [
    "## Save All Final Models and Tokenizers (RoBERTa + T5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70215e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final model save directory\n",
    "final_models_dir = \"./saved_models/final_combined\"\n",
    "os.makedirs(final_models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e92026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models explicitly from saved paths\n",
    "chat_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_response_generator\")\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_qa\")\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"./saved_models/emotion_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4720af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load respective tokenizers\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "237e7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to final directory\n",
    "chat_model.save_pretrained(os.path.join(final_models_dir, \"chat_model\"))\n",
    "qa_model.save_pretrained(os.path.join(final_models_dir, \"qa_model\"))\n",
    "emotion_model.save_pretrained(os.path.join(final_models_dir, \"emotion_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61f771bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and tokenizers have been saved successfully in './saved_models/final_combined'.\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizers explicitly\n",
    "t5_tokenizer.save_pretrained(os.path.join(final_models_dir, \"t5_tokenizer\"))\n",
    "emotion_tokenizer.save_pretrained(os.path.join(final_models_dir, \"emotion_tokenizer\"))\n",
    "\n",
    "print(f\"All models and tokenizers have been saved successfully in '{final_models_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753565",
   "metadata": {},
   "source": [
    "## Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7084f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    def __init__(self, emo_path, qa_path, resp_path, labels):\n",
    "        self.emo_path = emo_path\n",
    "        self.qa_path = qa_path\n",
    "        self.resp_path = resp_path\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b031591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_config.pt written to ./saved_models/final_combined/\n"
     ]
    }
   ],
   "source": [
    "# Save pipeline_config.pt only if all paths and emotion_labels exist\n",
    "if 'emotion_labels' in globals():\n",
    "    config = PipelineConfig(\n",
    "        emo_path=\"./saved_models/emotion_classifier\",\n",
    "        qa_path=\"./saved_models/t5_qa\",\n",
    "        resp_path=\"./saved_models/t5_response\",\n",
    "        labels=emotion_labels\n",
    "    )\n",
    "\n",
    "    final_dir = Path(\"./saved_models/final_combined\")\n",
    "    final_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(config, final_dir / \"pipeline_config.pt\")\n",
    "    print(\"pipeline_config.pt written to ./saved_models/final_combined/\")\n",
    "else:\n",
    "    print(\"Could not save pipeline_config.pt: 'emotion_labels' is undefined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93d4ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, emo_model, emo_tokenizer, qa_model, qa_tokenizer, response_model, response_tokenizer, labels):\n",
    "        self.emo_model = emo_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "        self.emo_tokenizer = emo_tokenizer\n",
    "        self.qa_model = qa_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "        self.qa_tokenizer = qa_tokenizer\n",
    "        self.response_model = response_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "        self.response_tokenizer = response_tokenizer\n",
    "        self.labels = labels\n",
    "\n",
    "    def analyze(self, text):\n",
    "        device = next(self.emo_model.parameters()).device\n",
    "\n",
    "        emo_inputs = self.emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.emo_model(**emo_inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().tolist()\n",
    "        emotions = [self.labels[i] for i, p in enumerate(probs) if p > 0.5]\n",
    "\n",
    "        qa_input = f\"question: {text}\"\n",
    "        qa_inputs = self.qa_tokenizer(qa_input, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            qa_ids = self.qa_model.generate(qa_inputs[\"input_ids\"], max_length=128)\n",
    "        answer = self.qa_tokenizer.decode(qa_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        prompt = f\"question: {text} context: {answer} emotions: {', '.join(emotions)}\"\n",
    "        response_inputs = self.response_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            response_ids = self.response_model.generate(response_inputs[\"input_ids\"], max_length=128)\n",
    "        response = self.response_tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return {\n",
    "            \"emotions\": emotions,\n",
    "            \"answer\": answer,\n",
    "            \"response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aea775fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./saved_models/final_combined/pipeline_config.pt\"\n",
    "config = torch.load(config_path, weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1846f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_model = AutoModelForSequenceClassification.from_pretrained(config.emo_path)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(config.emo_path)\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(config.qa_path)\n",
    "qa_tokenizer = T5Tokenizer.from_pretrained(config.qa_path)\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(t5_response_path)\n",
    "resp_tokenizer = T5Tokenizer.from_pretrained(t5_response_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f572ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = MentalHealthChatbotPipeline(\n",
    "    emo_model=emo_model,\n",
    "    emo_tokenizer=emo_tokenizer,\n",
    "    qa_model=qa_model,\n",
    "    qa_tokenizer=qa_tokenizer,\n",
    "    response_model=resp_model,\n",
    "    response_tokenizer=resp_tokenizer,\n",
    "    labels=config.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d324d4",
   "metadata": {},
   "source": [
    "## Save Final Model Metadata (.pt) for Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cdf95ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model metadata saved successfully at './saved_models/final_model_metadata.pt'.\n"
     ]
    }
   ],
   "source": [
    "# Define path explicitly and ensure directory exists\n",
    "final_metadata_path = \"./saved_models/final_model_metadata.pt\"\n",
    "os.makedirs(os.path.dirname(final_metadata_path), exist_ok=True)\n",
    "\n",
    "# Lightweight metadata dictionary (pointer-style)\n",
    "final_model_metadata = {\n",
    "    \"chat_model_path\": \"./saved_models/final/chat_model\",\n",
    "    \"qa_model_path\": \"./saved_models/final/qa_model\",\n",
    "    \"emotion_model_path\": \"./saved_models/final/emotion_model\",\n",
    "    \"t5_tokenizer_path\": \"./saved_models/final/t5_tokenizer\",\n",
    "    \"emotion_tokenizer_path\": \"./saved_models/final/emotion_tokenizer\",\n",
    "    \"labels\": emotion_labels  # list of emotion label names only\n",
    "}\n",
    "\n",
    "# Save the metadata dictionary\n",
    "torch.save(final_model_metadata, final_metadata_path)\n",
    "\n",
    "print(f\"Final model metadata saved successfully at '{final_metadata_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c89652",
   "metadata": {},
   "source": [
    "## Emotion Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c03dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotions: []\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"I feel like I'm breaking down and can't handle anything.\"\n",
    "emo_inputs = emo_tokenizer(sample_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "emo_inputs = {k: v.to(device) for k, v in emo_inputs.items()}\n",
    "\n",
    "emo_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = emo_model(**emo_inputs).logits\n",
    "    probs = torch.sigmoid(logits).squeeze().cpu().tolist()\n",
    "    predicted_emotions = [emotion_labels[i] for i, p in enumerate(probs) if p > 0.5]\n",
    "\n",
    "print(\"Detected Emotions:\", predicted_emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ef1c8",
   "metadata": {},
   "source": [
    "## T5 QA Model Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "797d6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Model Response: \n"
     ]
    }
   ],
   "source": [
    "qa_input = \"question: What are some ways to manage daily anxiety?\"\n",
    "qa_inputs = tokenizer_t5(qa_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "qa_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = qa_model.generate(qa_inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "response = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"QA Model Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051945db",
   "metadata": {},
   "source": [
    "## Safe Config Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e10f0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# If needed: torch.serialization.add_safe_globals([PipelineConfig])  # define class first\n",
    "torch.serialization.add_safe_globals([\"__main__.PipelineConfig\"])\n",
    "\n",
    "config_path = \"./saved_models/final_combined/pipeline_config.pt\"\n",
    "config = torch.load(config_path, weights_only=False)\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(config.emo_path).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(config.emo_path)\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(config.qa_path).to(device)\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(config.qa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264933a",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics and Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8078a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Prepare sample data\n",
    "sample_size = min(100, len(test_ds))\n",
    "sample = test_ds.select(range(sample_size))\n",
    "\n",
    "def generate_predictions(model, tokenizer, dataset):\n",
    "    inputs = [f\"question: {x['question']}\" for x in dataset]\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=128)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    refs = [x[\"answer\"] for x in dataset]\n",
    "    return preds, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a908af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "BERTScore: {'precision': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'recall': [0.7789900898933411, 0.7745112776756287, 0.7828745245933533, 0.8006718158721924, 0.7667760848999023, 0.7574717998504639, 0.7806887626647949, 0.7834129333496094, 0.7503082752227783, 0.7855865955352783, 0.7836850881576538, 0.7401551008224487, 0.7692136168479919, 0.7965372800827026, 0.7937134504318237, 0.7801406979560852, 0.7795731425285339, 0.8081868886947632], 'f1': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.9(hug_trans=4.49.0)'}\n",
      "Accuracy: {'accuracy': 1.0}\n",
      "Precision: {'precision': 0.0}\n",
      "Recall: {'recall': 0.0}\n",
      "F1 Score: {'f1': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions, references = generate_predictions(model_t5_qa, tokenizer_t5, sample)\n",
    "\n",
    "# Evaluate\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# Binary approximation for classification scores\n",
    "true_labels = [1 if r.strip().lower() in p.strip().lower() else 0 for p, r in zip(predictions, references)]\n",
    "accuracy_result = accuracy.compute(references=true_labels, predictions=true_labels)\n",
    "precision_result = precision.compute(references=true_labels, predictions=true_labels)\n",
    "recall_result = recall.compute(references=true_labels, predictions=true_labels)\n",
    "f1_result = f1.compute(references=true_labels, predictions=true_labels)\n",
    "\n",
    "# Display\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "print(\"BERTScore:\", bertscore_results)\n",
    "print(\"Accuracy:\", accuracy_result)\n",
    "print(\"Precision:\", precision_result)\n",
    "print(\"Recall:\", recall_result)\n",
    "print(\"F1 Score:\", f1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec6f8031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Detected Emotions: []\n",
      "QA Model Response: \n"
     ]
    }
   ],
   "source": [
    "# RoBERTa Emotion Classifier - Inference Test\n",
    "sample_input = \"I feel like I'm breaking down and can't handle anything.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Ensure all tensors are on the same device\n",
    "inputs = tokenizer_emo(sample_input, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "model_emo.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model_emo(**inputs).logits\n",
    "probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "\n",
    "predicted_emotions = [emotion_labels[i] for i, p in enumerate(probs) if p > 0.5]\n",
    "print(\"ðŸ” Detected Emotions:\", predicted_emotions)\n",
    "\n",
    "# T5 QA Model - Inference Test\n",
    "qa_input = \"question: What are some ways to manage daily anxiety?\"\n",
    "inputs = tokenizer_t5(qa_input, return_tensors=\"pt\").to(device)  # Move inputs to the same device\n",
    "qa_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = qa_model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "response = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"QA Model Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5b3e6",
   "metadata": {},
   "source": [
    "## Gradio Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2c9de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\gradio\\components\\base.py:423: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  obj = utils.component_or_layout_class(cls_name)(render=render)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def gradio_chat_interface(user_input):\n",
    "    global chat_history\n",
    "    response_data = generate_combined_response(user_input)\n",
    "    chatbot_response = response_data['Response']\n",
    "    chat_history.append((\"You: \" + user_input, \"Bot: \" + chatbot_response))\n",
    "    return chat_history\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_chat_interface,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"chatbot\",\n",
    "    title=\"Mental Health Chatbot\",\n",
    "    description=\"Emotion-aware chatbot using RoBERTa + T5\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
