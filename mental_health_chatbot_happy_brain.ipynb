{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a79395fe",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Happy Brain Mental Health Chatbot Training Notebook\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Set device\n",
    "# Force selection of the NVIDIA GPU (assumed as device 0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10e1b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Hugging Face...\n",
      "MH FAQ dataset loaded from HF\n"
     ]
    }
   ],
   "source": [
    "print('Loading datasets from Hugging Face...')\n",
    "\n",
    "# Mental Health FAQ dataset from HF\n",
    "try:\n",
    "    mh_faq = load_dataset('csv', data_files='hf://datasets/tolu07/Mental_Health_FAQ/Mental_Health_FAQ.csv')\n",
    "    print('MH FAQ dataset loaded from HF')\n",
    "except Exception as e:\n",
    "    print('Error loading MH FAQ dataset from HF:', e)\n",
    "    mh_faq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c15c021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental Health Counseling Conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# Mental Health Counseling Conversations\n",
    "try:\n",
    "    mh_counseling = load_dataset('Amod/mental_health_counseling_conversations')\n",
    "    print('Mental Health Counseling Conversations loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading Mental Health Counseling Conversations:', e)\n",
    "    mh_counseling = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f4dba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Medical Chatbot dialogues loaded\n"
     ]
    }
   ],
   "source": [
    "# AI Medical Chatbot dialogues\n",
    "try:\n",
    "    ai_medical = load_dataset('parquet', data_files='hf://datasets/ruslanmv/ai-medical-chatbot/dialogues.parquet')\n",
    "    print('AI Medical Chatbot dialogues loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading AI Medical Chatbot dialogues:', e)\n",
    "    ai_medical = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5407b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatDoctor dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# ChatDoctor-HealthCareMagic dataset\n",
    "try:\n",
    "    chatdoctor = load_dataset('parquet', data_files='hf://datasets/lavita/ChatDoctor-HealthCareMagic-100k/data/train-00000-of-00001-5e7cb295b9cff0bf.parquet')\n",
    "    print('ChatDoctor dataset loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading ChatDoctor dataset:', e)\n",
    "    chatdoctor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c054bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental Health Chatbot dataset loaded from HF\n"
     ]
    }
   ],
   "source": [
    "# Mental Health Chatbot dataset from HF\n",
    "try:\n",
    "    mh_chatbot = load_dataset('parquet', data_files='hf://datasets/heliosbrahma/mental_health_chatbot_dataset/data/train-00000-of-00001-01391a60ef5c00d9.parquet')\n",
    "    print('Mental Health Chatbot dataset loaded from HF')\n",
    "except Exception as e:\n",
    "    print('Error loading Mental Health Chatbot dataset from HF:', e)\n",
    "    mh_chatbot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6076952",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# List the datasets for further processing\n",
    "hf_datasets = [\n",
    "    (mh_faq, 'mh_faq'),\n",
    "    (mh_counseling, 'mh_counseling'),\n",
    "    (ai_medical, 'ai_medical'),\n",
    "    (chatdoctor, 'chatdoctor'),\n",
    "    (mh_chatbot, 'mh_chatbot')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23f89f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local CSV files...\n",
      "Local Mental Health FAQ CSV loaded, columns: ['Question_ID', 'Questions', 'Answers']\n"
     ]
    }
   ],
   "source": [
    "print('Loading local CSV files...')\n",
    "\n",
    "# Load Mental Health FAQ local CSV\n",
    "try:\n",
    "    local_mh_faq = pd.read_csv('./data/mental_health_faq.csv', encoding='utf-8')\n",
    "    print('Local Mental Health FAQ CSV loaded, columns:', list(local_mh_faq.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading local Mental Health FAQ CSV:', e)\n",
    "    local_mh_faq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8285140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV loaded, columns: ['User Input', 'Friend Mode Response', 'Professional Mode Response']\n"
     ]
    }
   ],
   "source": [
    "# Load the Friend mode and Professional mode Responses CSV\n",
    "try:\n",
    "    responses = pd.read_csv('./data/Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv', encoding='utf-8')\n",
    "    print('Responses CSV loaded, columns:', list(responses.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading Responses CSV:', e)\n",
    "    responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bde4ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local MH FAQ cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Clean local_mh_faq: Separate into question and answer columns if possible\n",
    "if local_mh_faq is not None:\n",
    "    # Attempt to find columns that contain 'question' and 'answer'\n",
    "    possible_q = [col for col in local_mh_faq.columns if 'question' in col.lower()]\n",
    "    possible_a = [col for col in local_mh_faq.columns if 'answer' in col.lower()]\n",
    "\n",
    "    if possible_q and possible_a:\n",
    "        local_mh_faq['text'] = 'User: ' + local_mh_faq[possible_q[0]].astype(str) + '\\nAssistant: ' + local_mh_faq[possible_a[0]].astype(str)\n",
    "    else:\n",
    "        # If specific columns are not found, use all columns concatenated\n",
    "        if 'text' in local_mh_faq.columns:\n",
    "            local_mh_faq['text'] = local_mh_faq['text']\n",
    "        else:\n",
    "            local_mh_faq['text'] = local_mh_faq.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Local MH FAQ cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV cleaned by concatenation.\n",
      "Local CSV files loading and cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean responses CSV: Create separate texts for friend mode and professional mode\n",
    "if responses is not None:\n",
    "    if 'User Input' in responses.columns and 'Friend Mode Response' in responses.columns and 'Professional Mode Response' in responses.columns:\n",
    "        responses['friend_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Friend Mode Response'].astype(str)\n",
    "        responses['professional_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Professional Mode Response'].astype(str)\n",
    "    else:\n",
    "        responses['text'] = responses.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Responses CSV cleaned by concatenation.')\n",
    "\n",
    "print('Local CSV files loading and cleaning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "699b2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading local Mental Health FAQ CSV: [Errno 2] No such file or directory: 'mental_health_faq.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load Mental Health FAQ local CSV\n",
    "try:\n",
    "    local_mh_faq = pd.read_csv('mental_health_faq.csv', encoding='utf-8')\n",
    "    print('Local Mental Health FAQ CSV loaded, columns:', list(local_mh_faq.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading local Mental Health FAQ CSV:', e)\n",
    "    local_mh_faq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Responses CSV: [Errno 2] No such file or directory: 'Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the Friend mode and Professional mode Responses CSV\n",
    "try:\n",
    "    responses = pd.read_csv('Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv', encoding='utf-8')\n",
    "    print('Responses CSV loaded, columns:', list(responses.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading Responses CSV:', e)\n",
    "    responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean local_mh_faq: Separate into question and answer columns if possible\n",
    "if local_mh_faq is not None:\n",
    "    # Attempt to find columns that contain 'question' and 'answer'\n",
    "    possible_q = [col for col in local_mh_faq.columns if 'question' in col.lower()]\n",
    "    possible_a = [col for col in local_mh_faq.columns if 'answer' in col.lower()]\n",
    "\n",
    "    if possible_q and possible_a:\n",
    "        local_mh_faq['text'] = 'User: ' + local_mh_faq[possible_q[0]].astype(str) + '\\nAssistant: ' + local_mh_faq[possible_a[0]].astype(str)\n",
    "    else:\n",
    "        # If specific columns are not found, use all columns concatenated\n",
    "        if 'text' in local_mh_faq.columns:\n",
    "            local_mh_faq['text'] = local_mh_faq['text']\n",
    "        else:\n",
    "            local_mh_faq['text'] = local_mh_faq.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Local MH FAQ cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local CSV files loading and cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean responses CSV: Create separate texts for friend mode and professional mode\n",
    "if responses is not None:\n",
    "    if 'User Input' in responses.columns and 'Friend Mode Response' in responses.columns and 'Professional Mode Response' in responses.columns:\n",
    "        responses['friend_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Friend Mode Response'].astype(str)\n",
    "        responses['professional_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Professional Mode Response'].astype(str)\n",
    "    else:\n",
    "        responses['text'] = responses.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Responses CSV cleaned by concatenation.')\n",
    "\n",
    "print('Local CSV files loading and cleaning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1001521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(example, source):\n",
    "    # For FAQ data, assume 'text' column exists\n",
    "    if source in ['mh_faq', 'local_mh_faq']:\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'mh_counseling':\n",
    "        if 'conversation' in example:\n",
    "            return example['conversation']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'ai_medical':\n",
    "        if 'dialogue' in example:\n",
    "            return example['dialogue']\n",
    "        elif 'text' in example:\n",
    "            return example['text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'chatdoctor':\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        elif 'instruction' in example and 'output' in example:\n",
    "            return 'User: ' + example['instruction'] + '\\nAssistant: ' + example['output']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'mh_chatbot':\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        elif 'conversation' in example:\n",
    "            return example['conversation']\n",
    "        elif 'question' in example and 'answer' in example:\n",
    "            return 'User: ' + example['question'] + '\\nAssistant: ' + example['answer']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'responses_friend':\n",
    "        if 'friend_text' in example:\n",
    "            return example['friend_text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'responses_professional':\n",
    "        if 'professional_text' in example:\n",
    "            return example['professional_text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    else:\n",
    "        return str(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0143652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to apply formatting to a dataset from Hugging Face\n",
    "def preprocess_dataset(dataset, source):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    split = 'train' if 'train' in dataset else list(dataset.keys())[0]\n",
    "    data = dataset[split]\n",
    "    formatted = data.map(lambda x: {'text': format_sample(x, source)})\n",
    "    return formatted\n",
    "\n",
    "print('Preprocessing function defined.')\n",
    "# Removed duplicate and improperly indented code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84d6ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added mh_faq to formatted datasets\n",
      "Added mh_counseling to formatted datasets\n",
      "Added ai_medical to formatted datasets\n",
      "Added chatdoctor to formatted datasets\n",
      "Added mh_chatbot to formatted datasets\n"
     ]
    }
   ],
   "source": [
    "formatted_datasets = []\n",
    "\n",
    "# Process datasets from HF\n",
    "for ds, source in hf_datasets:\n",
    "    if ds is not None:\n",
    "        formatted = preprocess_dataset(ds, source)\n",
    "        if formatted is not None:\n",
    "            formatted_datasets.append(formatted)\n",
    "            print('Added', source, 'to formatted datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7e78759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process local datasets\n",
    "if local_mh_faq is not None:\n",
    "    # Convert pandas DataFrame to Dataset\n",
    "    local_mh_faq_ds = Dataset.from_pandas(local_mh_faq[['text']])\n",
    "    formatted_datasets.append(local_mh_faq_ds)\n",
    "    print('Added local_mh_faq to formatted datasets')\n",
    "\n",
    "if responses is not None:\n",
    "    # Create two datasets: one for friend mode and one for professional mode\n",
    "    if 'friend_text' in responses.columns:\n",
    "        friend_ds = Dataset.from_pandas(responses[['friend_text']].rename(columns={'friend_text': 'text'}))\n",
    "        formatted_datasets.append(friend_ds)\n",
    "        print('Added responses_friend to formatted datasets')\n",
    "    \n",
    "    if 'professional_text' in responses.columns:\n",
    "        prof_ds = Dataset.from_pandas(responses[['professional_text']].rename(columns={'professional_text': 'text'}))\n",
    "        formatted_datasets.append(prof_ds)\n",
    "        print('Added responses_professional to formatted datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2904f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created with 372863 samples\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "if formatted_datasets:\n",
    "    combined_dataset = concatenate_datasets(formatted_datasets)\n",
    "    print('Combined dataset created with', len(combined_dataset), 'samples')\n",
    "else:\n",
    "    print('No datasets were successfully processed')\n",
    "    combined_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec2aa2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = 'gpt2'  # Base model to fine-tune\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6228ed57",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized\n",
      "Dataset split into 335576 training samples and 37287 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "if combined_dataset is not None:\n",
    "    tokenized_dataset = combined_dataset.map(tokenize_function, batched=True)\n",
    "    print('Dataset tokenized')\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.9 * len(tokenized_dataset))\n",
    "    val_size = len(tokenized_dataset) - train_size\n",
    "    \n",
    "    train_dataset = tokenized_dataset.select(range(train_size))\n",
    "    val_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "    \n",
    "    print('Dataset split into', len(train_dataset), 'training samples and', len(val_dataset), 'validation samples')\n",
    "else:\n",
    "    print('No dataset to tokenize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b81a65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./happy_brain',  # Use the new model name\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abf92f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ffdc8a7",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print('Training setup complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833a550",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='251682' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     2/251682 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./happy_brain')\n",
    "tokenizer.save_pretrained('./happy_brain')\n",
    "\n",
    "print('Model trained and saved as \"happy_brain\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a98f4",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "inference_model = AutoModelForCausalLM.from_pretrained('./happy_brain')\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained('./happy_brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = inference_tokenizer('User: ' + prompt + '\n",
    "Assistant:', return_tensors='pt')\n",
    "    outputs = inference_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=inference_tokenizer.eos_token_id\n",
    "    )\n",
    "    response = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    if 'Assistant:' in response:\n",
    "        response = response.split('Assistant:')[1].strip()\n",
    "    return response\n",
    "\n",
    "# Test the model with a few examples\n",
    "test_prompts = [\n",
    "    \"I'm feeling really anxious about my upcoming exam.\",\n",
    "    \"I've been feeling sad lately and I don't know why.\",\n",
    "    \"How can I improve my mental health?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(prompt)\n",
    "    print('User:', prompt)\n",
    "    print('Happy Brain:', response)\n",
    "    print('---')\n",
    "\n",
    "print('Inference test complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
