{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9254d42",
   "metadata": {},
   "source": [
    "\n",
    "# Mental Health Chatbot Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline to train a conversational mental health assistant. \n",
    "The system integrates emotion classification using `SamLowe/roberta-base-go_emotions` and text generation using `T5`.\n",
    "It processes multiple cleaned datasets, performs training, evaluation, and finally builds a chatbot interface using Gradio or Streamlit.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load and preprocess multiple mental health-related datasets into a consistent question/answer format\n",
    "- Simulate and encode multi-label emotion annotations using `MultiLabelBinarizer`\n",
    "- Train a RoBERTa-based emotion classification model with live metric logging (accuracy, F1, precision, recall)\n",
    "- Fine-tune two separate T5 models:\n",
    "  - One for emotionally guided chatbot response generation\n",
    "  - One for direct factual Q&A answering\n",
    "- Implement emotion-aware routing logic that selects the appropriate model at inference time\n",
    "- Create a unified RoBERTa + T5 pipeline for real-time response generation\n",
    "- Build a Gradio chatbot interface using the full system\n",
    "- Evaluate all models with live logging and inference testing\n",
    "- Save all models and tokenizers in `./saved_models/`\n",
    "- Save a `.pt` metadata file pointing to model paths for easy deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dff8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Hugging Face Datasets & Evaluation\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# External Evaluation Libraries\n",
    "import evaluate  # for rouge, bertscore, and other NLP metrics\n",
    "\n",
    "# Gradio for UI\n",
    "import gradio as gr\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional (remove unless explicitly required)\n",
    "# from accelerate import init_empty_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a8d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Current Device Index: 0\n",
      "Device Name: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Device setup (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check device information\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"Using CPU as GPU is not available.\")\n",
    "\n",
    "# Optional: set default tensor type to GPU-based FloatTensor if GPU is available\n",
    "# Only uncomment if required\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_dtype(torch.float32)\n",
    "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7328221",
   "metadata": {},
   "source": [
    "## Load and Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e48dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mode' column not found.\n",
      "Modified CSV saved and overwrote ./data/ds4_mental_health_chatbot_dataset_merged_modes.csv.\n",
      "Loaded dataset 'ds2' with 172 entries.\n",
      "Training dataset size: 154\n",
      "Test dataset size: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 154\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 18\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# Load and preprocess 'ds4'\n",
    "ds4_path = './data/ds4_mental_health_chatbot_dataset_merged_modes.csv'\n",
    "\n",
    "df = pd.read_csv(ds4_path)\n",
    "\n",
    "# Drop the \"mode\" column if it exists\n",
    "if 'mode' in df.columns:\n",
    "    df.drop(columns=['mode'], inplace=True)\n",
    "    print(\"Dropped 'mode' column.\")\n",
    "else:\n",
    "    print(\"'mode' column not found.\")\n",
    "\n",
    "# Overwrite original CSV explicitly\n",
    "df.to_csv(ds4_path, index=False)\n",
    "print(f\"Modified CSV saved and overwrote {ds4_path}.\")\n",
    "\n",
    "# File paths\n",
    "dataset_paths = {\n",
    "    \"ds1\": \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",\n",
    "    \"ds2\": \"./data/ds2_transformed_mental_health_chatbot.csv\",\n",
    "    \"ds3\": \"./data/ds3_mental_health_faq_cleaned.csv\",\n",
    "    \"ds4\": ds4_path,\n",
    "    \"ds5\": \"./data/ds5_Mental_Health_FAQ.csv\",\n",
    "    \"ds6\": \"./data/ds6_mental_health_counseling.csv\"\n",
    "}\n",
    "\n",
    "# Enable/disable datasets\n",
    "dataset_switches = {\n",
    "    \"ds1\": False,\n",
    "    \"ds2\": True,\n",
    "    \"ds3\": False,\n",
    "    \"ds4\": False,\n",
    "    \"ds5\": False,\n",
    "    \"ds6\": False\n",
    "}\n",
    "\n",
    "# Cleaning function\n",
    "def load_and_clean_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [col.lower().strip() for col in df.columns]\n",
    "\n",
    "    # Rename columns to standard format\n",
    "    if \"prompt\" in df.columns and \"response\" in df.columns:\n",
    "        df.rename(columns={\"prompt\": \"question\", \"response\": \"answer\"}, inplace=True)\n",
    "    if \"questions\" in df.columns:\n",
    "        df.rename(columns={\"questions\": \"question\"}, inplace=True)\n",
    "    if \"answers\" in df.columns:\n",
    "        df.rename(columns={\"answers\": \"answer\"}, inplace=True)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    df = df[[\"question\", \"answer\"]].dropna().reset_index(drop=True)\n",
    "    \n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Load selected datasets\n",
    "datasets_list = []\n",
    "for name, path in dataset_paths.items():\n",
    "    if dataset_switches.get(name, False):\n",
    "        dataset = load_and_clean_csv(path)\n",
    "        print(f\"Loaded dataset '{name}' with {len(dataset)} entries.\")\n",
    "        datasets_list.append(dataset)\n",
    "\n",
    "# Validate that at least one dataset is loaded\n",
    "if not datasets_list:\n",
    "    raise ValueError(\"No datasets selected. Please enable at least one dataset in 'dataset_switches'.\")\n",
    "\n",
    "# Merge and split dataset\n",
    "combined_dataset = concatenate_datasets(datasets_list).shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643608e6",
   "metadata": {},
   "source": [
    "## Prepare 'labels' Column with MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4287b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample emotion annotations (train): [['annoyance', 'admiration', 'realization'], ['curiosity', 'approval'], ['annoyance', 'optimism', 'realization']]\n",
      "Sample binarized labels (train): 0    [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1    [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = train_ds.to_pandas()\n",
    "df_test = test_ds.to_pandas()\n",
    "\n",
    "# Simulated emotion annotations (replace with real data if available)\n",
    "emotions_list = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "                 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "                 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "                 'relief', 'remorse', 'sadness', 'surprise']\n",
    "\n",
    "df_train[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_train))]\n",
    "df_test[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_test))]\n",
    "\n",
    "# Encode with MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=emotions_list)\n",
    "\n",
    "df_train[\"labels\"] = list(mlb.fit_transform(df_train[\"emotions\"]))\n",
    "df_test[\"labels\"] = list(mlb.transform(df_test[\"emotions\"]))\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Sample emotion annotations (train):\", df_train[\"emotions\"].iloc[:3].tolist())\n",
    "print(\"Sample binarized labels (train):\", df_train[\"labels\"].iloc[:3])\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "test_ds = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d45158",
   "metadata": {},
   "source": [
    "## Configure RoBERTa for Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65350e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa multi-label classification model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "# Define RoBERTa configuration explicitly for multi-label classification\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=len(emotions_list)\n",
    ")\n",
    "\n",
    "# Load the RoBERTa model with the defined configuration\n",
    "model_emo = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    config=config\n",
    ").to(device)\n",
    "\n",
    "print(\"RoBERTa multi-label classification model loaded and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b245b",
   "metadata": {},
   "source": [
    "## Train RoBERTa with Correct Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fcc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b563f652252c44ae93ea6859ca2509ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e5b92298a749ed896bdd7bff62e0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104c3c3799754c7888102a99711e4096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0a263536304d32b18999ec325b321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_33776\\1231195333.py:110: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.231088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.239444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./saved_models/emotion_classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    RobertaConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Features, Value, Sequence\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Define model directory\n",
    "model_path = \"./saved_models/emotion_classifier\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_emo = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Load or initialize model\n",
    "if os.path.exists(os.path.join(model_path, 'config.json')):\n",
    "    print(\"Loading previously trained model...\")\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(model_path, config=config).to(device)\n",
    "else:\n",
    "    print(\"No existing model found. Initializing new model.\")\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(emotions_list)\n",
    "    )\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"emotions\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(Value(\"float32\"))\n",
    "})\n",
    "\n",
    "# Apply features format\n",
    "train_ds = train_ds.cast(features)\n",
    "test_ds = test_ds.cast(features)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_emotion(example):\n",
    "    enc = tokenizer_emo(\n",
    "        example[\"question\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128  # explicitly limit for efficiency\n",
    "    )\n",
    "    enc[\"labels\"] = example[\"labels\"]\n",
    "    return enc\n",
    "\n",
    "# Tokenize datasets\n",
    "train_emo = train_ds.map(tokenize_emotion, batched=False)\n",
    "test_emo = test_ds.map(tokenize_emotion, batched=False)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Data collator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_emo, return_tensors=\"pt\")\n",
    "\n",
    "# Metrics computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='micro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Training arguments setup\n",
    "training_args_emo = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_emo = Trainer(\n",
    "    model=model_emo,\n",
    "    args=training_args_emo,\n",
    "    train_dataset=train_emo,\n",
    "    eval_dataset=test_emo,\n",
    "    tokenizer=tokenizer_emo,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_emo.train()\n",
    "\n",
    "# Save the trained model explicitly\n",
    "trainer_emo.save_model(model_path)\n",
    "tokenizer_emo.save_pretrained(model_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7782",
   "metadata": {},
   "source": [
    "## Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa85bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing T5 model found. Loading 't5-small'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfc591bc0e9462285e673a3f1e37cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f8fb8082ef47c698d5031a0a9068d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets processed and ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_33776\\2789127582.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_chat = Trainer(\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\data\\data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.279200</td>\n",
       "      <td>1.709940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.414200</td>\n",
       "      <td>1.602948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.904200</td>\n",
       "      <td>1.573380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! T5 response-generation model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Define model save path\n",
    "t5_response_path = \"./saved_models/t5_response_generator\"\n",
    "os.makedirs(t5_response_path, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Cleanup invalid or empty model directory\n",
    "if os.path.exists(t5_response_path) and not any(\n",
    "    fname.endswith((\".bin\", \".safetensors\", \".h5\", \".index\", \".msgpack\"))\n",
    "    for fname in os.listdir(t5_response_path)\n",
    "):\n",
    "    print(\"Empty or invalid model directory detected. Removing...\")\n",
    "    shutil.rmtree(t5_response_path)\n",
    "    os.makedirs(t5_response_path, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load or initialize T5 model explicitly on the correct device\n",
    "if os.path.exists(os.path.join(t5_response_path, \"pytorch_model.bin\")):\n",
    "    print(\"Loading existing trained T5 model...\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(t5_response_path).to(device)\n",
    "else:\n",
    "    print(\"No existing T5 model found. Loading 't5-small'.\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Tokenization function for T5 chat model\n",
    "def tokenize_t5_chat(example):\n",
    "    input_text = \"chat: \" + example[\"question\"]\n",
    "    target_text = example[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer_t5(\n",
    "        target_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).input_ids\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs  \n",
    "\n",
    "# Process datasets\n",
    "train_chat = train_ds.map(tokenize_t5_chat, batched=False)\n",
    "test_chat = test_ds.map(tokenize_t5_chat, batched=False)\n",
    "\n",
    "# Set dataset format explicitly\n",
    "train_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Datasets processed and ready for training!\")\n",
    "\n",
    "# Data collator for seq2seq tasks (recommended for T5)\n",
    "data_collator_seq2seq = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=model_t5_response,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args_chat = TrainingArguments(\n",
    "    output_dir=t5_response_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    save_safetensors=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Trainer setup and training\n",
    "trainer_chat = Trainer(\n",
    "    model=model_t5_response,\n",
    "    args=training_args_chat,\n",
    "    train_dataset=train_chat,\n",
    "    eval_dataset=test_chat,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator_seq2seq\n",
    ")\n",
    "\n",
    "# Begin training\n",
    "trainer_chat.train()\n",
    "\n",
    "# Save model and tokenizer explicitly\n",
    "trainer_chat.save_model(t5_response_path)\n",
    "tokenizer_t5.save_pretrained(t5_response_path)\n",
    "\n",
    "print(\"Training complete! T5 response-generation model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcdc2d",
   "metadata": {},
   "source": [
    "## Train T5 for Q&A Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ca64ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing Q&A model found. Starting from 't5-small'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282e243a5e254bcfaf90a358a67f9c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c3c173fc0f418d8411831619ad9448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_33776\\3986052619.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_qa = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 00:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.915700</td>\n",
       "      <td>1.882983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.248400</td>\n",
       "      <td>1.759358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.774300</td>\n",
       "      <td>1.722508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! T5 Q&A model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "# Metric setup\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define model save path\n",
    "t5_qa_path = \"./saved_models/t5_qa\"\n",
    "os.makedirs(t5_qa_path, exist_ok=True)\n",
    "\n",
    "# Load existing model or initialize new\n",
    "if os.path.exists(os.path.join(t5_qa_path, \"pytorch_model.bin\")):\n",
    "    print(\"Loading previously trained T5 Q&A model...\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(t5_qa_path).to(device)\n",
    "else:\n",
    "    print(\"No existing Q&A model found. Starting from 't5-small'.\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_t5_qa(examples):\n",
    "    input_texts = [\"question: \" + q for q in examples[\"question\"]]\n",
    "    target_texts = examples[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(\n",
    "            target_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "        ).input_ids\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Tokenize datasets\n",
    "train_qa = train_ds.map(tokenize_t5_qa, batched=True)\n",
    "test_qa = test_ds.map(tokenize_t5_qa, batched=True)\n",
    "\n",
    "train_qa.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_qa.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Data collator\n",
    "data_collator_qa = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=model_t5_qa,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Unpack predictions if necessary\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predictions = [list(p) if isinstance(p, (np.ndarray, torch.Tensor)) else p for p in predictions]\n",
    "\n",
    "    # Ensure labels is a list of lists\n",
    "    if isinstance(labels[0], (int, np.integer)):\n",
    "        labels = [labels]\n",
    "    else:\n",
    "        labels = [list(l) if isinstance(l, (np.ndarray, torch.Tensor)) else l for l in labels]\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    labels = [[tokenizer_t5.pad_token_id if token == -100 else token for token in label] for label in labels]\n",
    "\n",
    "    # Decode\n",
    "    decoded_preds = tokenizer_t5.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {k: v for k, v in result.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args_qa = Seq2SeqTrainingArguments(\n",
    "    output_dir=t5_qa_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    seed=42,\n",
    "    predict_with_generate=True  # ðŸ‘ˆ key for Seq2Seq generation\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=model_t5_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=train_qa,\n",
    "    eval_dataset=test_qa,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator_qa,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and save\n",
    "trainer_qa.train()\n",
    "trainer_qa.save_model(t5_qa_path)\n",
    "tokenizer_t5.save_pretrained(t5_qa_path)\n",
    "\n",
    "print(\"Training complete! T5 Q&A model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0b4f8",
   "metadata": {},
   "source": [
    "## Unified Emotion-Aware Response System (RoBERTa + T5 Routing Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456d4c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Detected Emotions': [], 'Model Used': 'QA Model', 'Response': ''}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification, RobertaTokenizer,\n",
    "    T5ForConditionalGeneration, T5Tokenizer\n",
    ")\n",
    "\n",
    "# Load models and tokenizers explicitly on the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "emotion_model_path = \"./saved_models/emotion_classifier\"\n",
    "t5_chat_model_path = \"./saved_models/t5_response_generator\"\n",
    "t5_qa_model_path = \"./saved_models/t5_qa\"\n",
    "\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(emotion_model_path).to(device)\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "t5_chat_model = T5ForConditionalGeneration.from_pretrained(t5_chat_model_path).to(device)\n",
    "t5_qa_model = T5ForConditionalGeneration.from_pretrained(t5_qa_model_path).to(device)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Emotion labels from GoEmotions dataset\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise'\n",
    "]\n",
    "\n",
    "# Emotion detection function\n",
    "def detect_emotions(text, threshold=0.5):\n",
    "    emotion_model.eval()\n",
    "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().tolist()\n",
    "    detected = [(emotion_labels[i], p) for i, p in enumerate(probs) if p > threshold]\n",
    "    return [label for label, _ in detected]\n",
    "\n",
    "# Dynamic routing for response generation\n",
    "def generate_combined_response(user_input):\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    emotional_keywords = {\n",
    "        'joy', 'sadness', 'anger', 'fear', 'love', 'grief', 'remorse',\n",
    "        'disappointment', 'gratitude', 'caring'\n",
    "    }\n",
    "    use_chat_model = any(e in emotional_keywords for e in emotions)\n",
    "\n",
    "    if use_chat_model:\n",
    "        prefix = \"chat: \"\n",
    "        model = t5_chat_model\n",
    "    else:\n",
    "        prefix = \"question: \"\n",
    "        model = t5_qa_model\n",
    "\n",
    "    context = f\"{' '.join(emotions)}: {user_input}\" if emotions else user_input\n",
    "    input_text = prefix + context\n",
    "\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "    response = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"Detected Emotions\": emotions,\n",
    "        \"Model Used\": \"Chat Model\" if use_chat_model else \"QA Model\",\n",
    "        \"Response\": response\n",
    "    }\n",
    "\n",
    "# Test example\n",
    "test_result = generate_combined_response(\"I feel really hopeless and angry all the time.\")\n",
    "print(test_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b38b07",
   "metadata": {},
   "source": [
    "## Save All Final Models and Tokenizers (RoBERTa + T5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21eb93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models and tokenizers have been saved successfully in './saved_models/final_combined'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification, RobertaTokenizer,\n",
    "    T5ForConditionalGeneration, T5Tokenizer\n",
    ")\n",
    "\n",
    "# Define final model save directory\n",
    "final_models_dir = \"./saved_models/final_combined\"\n",
    "os.makedirs(final_models_dir, exist_ok=True)\n",
    "\n",
    "# Load trained models explicitly from saved paths\n",
    "chat_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_response_generator\")\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_qa\")\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"./saved_models/emotion_classifier\")\n",
    "\n",
    "# Load respective tokenizers\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Save models to final directory\n",
    "chat_model.save_pretrained(os.path.join(final_models_dir, \"chat_model\"))\n",
    "qa_model.save_pretrained(os.path.join(final_models_dir, \"qa_model\"))\n",
    "emotion_model.save_pretrained(os.path.join(final_models_dir, \"emotion_model\"))\n",
    "\n",
    "# Save tokenizers explicitly\n",
    "t5_tokenizer.save_pretrained(os.path.join(final_models_dir, \"t5_tokenizer\"))\n",
    "emotion_tokenizer.save_pretrained(os.path.join(final_models_dir, \"emotion_tokenizer\"))\n",
    "\n",
    "print(f\"All models and tokenizers have been saved successfully in '{final_models_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff046f",
   "metadata": {},
   "source": [
    "## Save Final Model Metadata (.pt) for Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83888a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model metadata saved successfully at './saved_models/final_model_metadata.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define path explicitly and ensure directory exists\n",
    "final_metadata_path = \"./saved_models/final_model_metadata.pt\"\n",
    "os.makedirs(os.path.dirname(final_metadata_path), exist_ok=True)\n",
    "\n",
    "# Lightweight metadata dictionary (pointer-style)\n",
    "final_model_metadata = {\n",
    "    \"chat_model_path\": \"./saved_models/final/chat_model\",\n",
    "    \"qa_model_path\": \"./saved_models/final/qa_model\",\n",
    "    \"emotion_model_path\": \"./saved_models/final/emotion_model\",\n",
    "    \"t5_tokenizer_path\": \"./saved_models/final/t5_tokenizer\",\n",
    "    \"emotion_tokenizer_path\": \"./saved_models/final/emotion_tokenizer\",\n",
    "    \"labels\": emotion_labels  # list of emotion label names only\n",
    "}\n",
    "\n",
    "# Save the metadata dictionary\n",
    "torch.save(final_model_metadata, final_metadata_path)\n",
    "\n",
    "print(f\"Final model metadata saved successfully at '{final_metadata_path}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f42cce",
   "metadata": {},
   "source": [
    "## Gradio Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b85f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_33776\\1799456066.py:16: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  outputs=[gr.Chatbot(label=\"Chat History\"), gr.State()],\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\gradio\\interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Chatbot interface function\n",
    "def gradio_chat_interface(user_input, history):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    response_data = generate_combined_response(user_input)\n",
    "    chatbot_response = response_data['Response']\n",
    "    history.append((user_input, chatbot_response))\n",
    "    return history, history\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_chat_interface,\n",
    "    inputs=[gr.Textbox(label=\"Your message\"), gr.State()],\n",
    "    outputs=[gr.Chatbot(label=\"Chat History\"), gr.State()],\n",
    "    title=\"Mental Health Chatbot - Happy Brain\",\n",
    "    description=\"Emotion-aware chatbot using RoBERTa + T5\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609a1ec",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics and Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9be7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "BERTScore: {'precision': 0.0, 'recall': 0.7806296116775937, 'f1': 0.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m emotion_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 42\u001b[0m     logits \u001b[38;5;241m=\u001b[39m emotion_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     43\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     45\u001b[0m     predicted_emotions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     46\u001b[0m         emotion_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(probs) \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     47\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1320\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1320\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1332\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:912\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 912\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:122\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    119\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    125\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Prepare sample data\n",
    "sample_size = min(100, len(test_ds))\n",
    "sample = test_ds.select(range(sample_size))\n",
    "\n",
    "# Function to generate predictions\n",
    "def generate_predictions(model, tokenizer, dataset, device):\n",
    "    inputs = [f\"question: {x['question']}\" for x in dataset]\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=128)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    refs = [x[\"answer\"] for x in dataset]\n",
    "    return preds, refs\n",
    "\n",
    "# Generate predictions for evaluation\n",
    "predictions, references = generate_predictions(model_t5_qa, tokenizer_t5, sample, device)\n",
    "\n",
    "# Evaluate using ROUGE and BERTScore\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "print(\"BERTScore:\", {k: np.mean(v) for k, v in bertscore_results.items() if k != 'hashcode'})\n",
    "\n",
    "# RoBERTa Emotion Classifier - Inference Test\n",
    "sample_input = \"I feel like I'm breaking down and can't handle anything.\"\n",
    "\n",
    "# Properly tokenize for RoBERTa and move to device\n",
    "inputs = emotion_tokenizer(sample_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "emotion_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = emotion_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).squeeze().cpu().tolist()\n",
    "\n",
    "    predicted_emotions = [\n",
    "        emotion_labels[i] for i, p in enumerate(probs) if p > 0.5\n",
    "    ]\n",
    "\n",
    "print(\"Detected Emotions:\", predicted_emotions)\n",
    "\n",
    "# T5 QA Model - Inference Test\n",
    "qa_input = \"question: What are some ways to manage daily anxiety?\"\n",
    "inputs = tokenizer_t5(qa_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "qa_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = qa_model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "response = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"QA Model Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
