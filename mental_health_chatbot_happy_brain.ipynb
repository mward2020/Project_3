{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Happy Brain: End-to-End Mental Health Chatbot\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Happy Brain** is an end-to-end mental health chatbot built from three independently fine-tuned NLP models. Each model serves a distinct purposeâ€”emotion recognition, factual answering, or therapeutic text generation. These models are unified through a dynamic routing system that selects the most contextually appropriate response strategy based on the user's input and detected emotional tone.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Components and Workflow\n",
    "\n",
    "### 2.1 Library Imports & Environment Setup\n",
    "- Uses essential libraries: `torch`, `transformers`, `datasets`, `sklearn`, `gradio`, etc.\n",
    "- Enforces CPU usage for compatibility and reproducibility.\n",
    "- Sets consistent random seed.\n",
    "- Creates a root directory (`SAVE_ROOT`) to store all models and metadata.\n",
    "\n",
    "### 2.2 Data Loading & Preprocessing\n",
    "- Loads and merges multiple mental health-related datasets.\n",
    "- Cleans and standardizes into uniform `question â†’ answer` format.\n",
    "- Tokenizes input using the corresponding tokenizer (RoBERTa, T5).\n",
    "- Splits datasets into training and testing sets.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Descriptions & Training\n",
    "\n",
    "### 3.1 Emotion Classification â€” Model 1\n",
    "- **Base Model:** `SamLowe/roberta-base-go_emotions`\n",
    "- **Objective:** Multi-label emotion detection tailored for mental health.\n",
    "- **Training:**\n",
    "  - Classification head adjusted for multiple emotion labels.\n",
    "  - Sigmoid activation with BCE loss.\n",
    "  - Optimized using AdamW (LR=2e-5).\n",
    "  - Implements early stopping and best-model checkpointing.\n",
    "- **Saving:** Robust saving with retry logic to prevent Windows file-locking issues.\n",
    "\n",
    "### 3.2 Supportive Response Generation â€” Model 2\n",
    "- **Base Model:** `google/flan-t5-large`\n",
    "- **Objective:** Generate emotionally supportive responses.\n",
    "- **Tokenization & Training:**\n",
    "  - Input: max 128 tokens; Output: max 64 tokens.\n",
    "  - Beam search decoding with no-repeat n-grams (size 2).\n",
    "- **Evaluation:** ROUGE-L and optional BERTScore.\n",
    "- **Saving:** Uses temporary file saves for compatibility.\n",
    "\n",
    "### 3.3 Factual Question Answering â€” Model 3\n",
    "- **Base Model:** `google/flan-t5-base`\n",
    "- **Objective:** Answer factual mental health-related questions.\n",
    "- **Training & Saving:** Follows same protocol as Model 2.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Pipeline Integration & Routing Logic\n",
    "\n",
    "### 4.1 Unified Metadata Loader\n",
    "- Stores all model paths and configuration in `combined_model_metadata.pt`.\n",
    "\n",
    "### 4.2 Dynamic Inference Pipeline\n",
    "- **Emotion Detection:**  \n",
    "  Uses RoBERTa model via `detect_emotions(text)` with threshold = 0.3.\n",
    "\n",
    "- **Routing Strategy:**\n",
    "  - If input includes a question mark â†’ QA model (T5-base).\n",
    "  - Otherwise, based on detected emotions â†’ Supportive model (T5-large).\n",
    "\n",
    "- **Prompt Construction:**  \n",
    "  Leverages detected emotions and conversation history.\n",
    "\n",
    "- **Standardized Generation Parameters:**\n",
    "  ```python\n",
    "  model.generate(\n",
    "      input_ids=...,\n",
    "      attention_mask=...,\n",
    "      max_length=100,\n",
    "      min_length=20,\n",
    "      num_beams=4,\n",
    "      no_repeat_ngram_size=2,\n",
    "      early_stopping=True\n",
    "  )\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Optimization Techniques Used\n",
    "\n",
    "Several optimization strategies were employed to enhance model performance and integration:\n",
    "\n",
    "- **Consistent Tokenization:**  \n",
    "  Uniform truncation and padding practices across all models to ensure coherent data processing.\n",
    "\n",
    "- **Multi-Label Scoring:**  \n",
    "  Use of Micro F1 score and threshold optimization for robust emotion classification.\n",
    "\n",
    "- **Early Stopping:**  \n",
    "  Retains the best-performing model checkpoint during training to prevent overfitting and improve generalization.\n",
    "\n",
    "- **Temporary Save Logic:**  \n",
    "  Implements a retry mechanism for file-saving to mitigate file-locking issues, particularly on Windows systems.\n",
    "\n",
    "- **LoRA Compatibility (Planned):**  \n",
    "  A modular design enabling future incorporation of Low-Rank Adaptation for efficient fine-tuning and memory optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Evaluation Summary\n",
    "\n",
    "A comprehensive evaluation framework ensures each model operates effectively within the chatbot pipeline:\n",
    "\n",
    "### 6.1 Emotion Classification\n",
    "\n",
    "Evaluated using micro-averaged metrics including:\n",
    "\n",
    "- **F1 Score**  \n",
    "- **Precision**  \n",
    "- **Recall**  \n",
    "- **Subset Accuracy**  \n",
    "\n",
    "These metrics quantify the accuracy and reliability of multi-label emotion predictions.\n",
    "\n",
    "### 6.2 Supportive Response Generation and Question-Answering\n",
    "\n",
    "Evaluated through text-generation metrics including:\n",
    "\n",
    "- **ROUGE-L:**  \n",
    "  Assesses the fluency and accuracy by measuring textual overlap between generated and reference texts.\n",
    "\n",
    "- **Perplexity:**  \n",
    "  Reflects model coherence and confidence based on the evaluation loss.\n",
    "\n",
    "- **BERTScore (Optional):**  \n",
    "  Measures semantic similarity between predictions and references for nuanced quality assurance.\n",
    "\n",
    "> Collectively, these evaluation techniques validate the chatbot's capability for emotionally intelligent and contextually appropriate interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "The **Happy Brain** chatbot integrates three specialized NLP models:\n",
    "\n",
    "- `SamLowe/roberta-base-go_emotions` â†’ Emotion detection  \n",
    "- `google/flan-t5-base` â†’ Factual QA  \n",
    "- `google/flan-t5-large` â†’ Supportive response generation\n",
    "\n",
    "These are merged into a cohesive, modular pipeline designed for nuanced, empathetic, and factual mental health conversations. Each model is trained, optimized, evaluated, and integrated with precisionâ€”empowering the chatbot to offer meaningful and emotionally aware user support.\n",
    "\n",
    "NOTE: The chatbot's responses are still mostly incorrect and require more refinement. Although emotion detection is generally accurate, Happy Brain is still a work in progress and needs much more training and refinement. This was a challenge considering the hardware and software limitations faced when creating a chatbot like this from (essentially) scratch in an educational setting. However, the chatbot is a great start and will provide a useful and empathetic platform for users seeking mental health support.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Environment Setup\n",
    "# ================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force CPU use\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ================================\n",
    "# Standard Library Imports\n",
    "# ================================\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import pprint  # Pretty-printing\n",
    "\n",
    "# ================================\n",
    "# Scientific & Data Libraries\n",
    "# ================================\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ================================\n",
    "# Audio Processing\n",
    "# ================================\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ================================\n",
    "# NLP & Transformers (Hugging Face)\n",
    "# ================================\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "import gradio as gr\n",
    "import streamlit as st\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Transformers - Tokenizers & Models\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "# Transformers - Training Tools\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "# Transformers - Logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# ================================\n",
    "# PEFT / LoRA\n",
    "# ================================\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ea550a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cpu\n",
      "FORCED device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "\n",
    "USE_CPU = True  # Set to True to force CPU mode\n",
    "device = torch.device('cpu' if USE_CPU else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Device in use:', device)\n",
    "\n",
    "# FORCE CPU for the entire session\n",
    "\n",
    "USE_CPU = True\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"FORCED device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Save directory setup\n",
    "\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"flan_t5_response_generator\", \"t5_qa\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b54c6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "390c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "\n",
    "DATASETS = {\n",
    "    \"ds1\": (True,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False, \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False, \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False, \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False, \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "1b472611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner that auto-maps columns to 'question' / 'answer'\n",
    "\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # Common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # If provided cols exist, rename them to standard names\n",
    "    \n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "    # Try to map 'context' -> 'question' if needed\n",
    "\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    # Verify if necessary columns exist\n",
    "\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    # Retain only required columns, drop missing values and duplicates\n",
    "\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    \n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "565109d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'ds1' from ./data/ds1_transformed_mental_health_chatbot_dataset.csv ...\n",
      "Loaded 172 examples from 'ds1'.\n",
      "Skipping dataset 'ds2' as its toggle is off.\n",
      "Skipping dataset 'ds3' as its toggle is off.\n",
      "Skipping dataset 'ds4' as its toggle is off.\n",
      "Skipping dataset 'ds5' as its toggle is off.\n",
      "Skipping dataset 'ds6' as its toggle is off.\n"
     ]
    }
   ],
   "source": [
    "# Load enabled datasets and create a unified dataset\n",
    "\n",
    "datasets = []\n",
    "for key, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if enabled:\n",
    "        print(f\"Loading dataset '{key}' from {path} ...\")\n",
    "        ds = load_and_clean(path, q_col, a_col)\n",
    "        print(f\"Loaded {len(ds)} examples from '{key}'.\")\n",
    "        datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Skipping dataset '{key}' as its toggle is off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "fd09483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined dataset contains 172 examples.\n",
      "Training set: 154 examples, Testing set: 18 examples.\n"
     ]
    }
   ],
   "source": [
    "# Create unified dataset\n",
    "\n",
    "if datasets:\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    print(f\"\\nCombined dataset contains {len(combined_dataset)} examples.\")\n",
    "else:\n",
    "    raise ValueError(\"No datasets enabled. Please enable at least one dataset in DATASETS.\")\n",
    "\n",
    "# Shuffle and split into training and testing datasets (e.g., 90% train, 10% test)\n",
    "\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "print(f\"Training set: {len(train_dataset)} examples, Testing set: {len(test_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228172",
   "metadata": {},
   "source": [
    "### Multi-Label Emotion Annotation & Custom Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "b8030b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed if not already defined\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Data Collator: casts labels to float32\n",
    "\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "\n",
    "    # Uncomment next line to print label info during debugging\n",
    "    # print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d3af2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for Multi-Label Classification\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that computes loss using binary crossâ€‘entropy with logits.\n",
    "    This ensures multiâ€‘label targets (e.g., emotions) are correctly processed.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # If labels' shape does not match logits, reshape to match\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "b0db1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback: Print training progress (optional)\n",
    "\n",
    "class StepPrinter(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Trainer callback that prints step-wise loss and evaluation metrics\n",
    "    while keeping the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} â€¢ loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" â€¢ metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" â€¢ eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "dfbe30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels (28 total: 27 + neutral)\n",
    "\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "1d31ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate each example with multi-label emotion annotations.\n",
    "# For this demo, we simulate emotion annotations: if an example has no emotion,\n",
    "# we default to \"neutral\". Replace this with your real emotion annotations if available.\n",
    "\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no explicit emotion annotations, default to [\"neutral\"]\n",
    "\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    # Save original emotions for visibility (optional)\n",
    "\n",
    "    example[\"emotions\"] = emos\n",
    "    \n",
    "    # Create a binary label vector for each of the 28 emotions\n",
    "    \n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "ce5a3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 154 examples â€¢ Test dataset: 18 examples\n"
     ]
    }
   ],
   "source": [
    "# Helper to retrieve input text from an example (for debugging)\n",
    "\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\"\n",
    "\n",
    "# (Reâ€‘)load datasets for emotion annotation if not already loaded.\n",
    "\n",
    "if 'train_dataset' not in globals() or 'test_dataset' not in globals():\n",
    "    print(\"Reloading datasets for emotion annotation ...\")\n",
    "    datasets_list = []\n",
    "    for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "        if not enabled:\n",
    "            continue\n",
    "        ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "        datasets_list.append(ds)\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "        fallback_data = {\n",
    "            \"text\": [\n",
    "                \"How are you?\",\n",
    "                \"I feel really down today.\",\n",
    "                \"I'm so happy with my progress!\",\n",
    "                \"Why does nobody understand me?\",\n",
    "                \"I'm feeling anxious about school.\",\n",
    "                \"Life is good lately.\",\n",
    "                \"Sometimes I just want to cry.\",\n",
    "                \"Everything is falling apart.\",\n",
    "                \"Iâ€™m grateful for my therapist.\",\n",
    "                \"Can someone please just listen?\"\n",
    "            ]\n",
    "        }\n",
    "        ds = Dataset.from_dict(fallback_data)\n",
    "        datasets_list.append(ds)\n",
    "    full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "    full_ds = full_ds.shuffle(seed=SEED)\n",
    "    split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} examples â€¢ Test dataset: {len(test_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "1f137d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a00df882a0c43b880d14682081c2d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251c700fb2654187bdd097596a79166d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample annotation:\n",
      "How does someone acquire a mental illness? -> ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# Map emotion annotations onto training and testing datasets\n",
    "\n",
    "emo_train = train_dataset.map(annotate_emotions)\n",
    "emo_test  = test_dataset.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample annotation:\")\n",
    "print(get_input_text(emo_train[0]), \"->\", emo_train[0][\"emotions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "59fbdc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c8e7e660b14eadb81965ed6aaca6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13d39fe378f4798a94388dc91bd9df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with problematic labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter out any examples without at least one positive label (shouldn't happen after defaulting to neutral)\n",
    "\n",
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n",
    "\n",
    "# Check for any problematic labels\n",
    "\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "print(\"Number of examples with problematic labels:\", len(bad_labels))\n",
    "if bad_labels:\n",
    "    print(\"Example with problematic labels:\", bad_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4b16533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before column rename: ['question', 'answer', 'emotions', 'labels']\n",
      "After column rename: ['text', 'answer', 'emotions', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for emotion classification.\n",
    "# We'll be using the 'SamLowe/roberta-base-go_emotions' tokenizer.\n",
    "\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Print columns before renaming for clarity\n",
    "\n",
    "print(\"Before column rename:\", emo_train.column_names)\n",
    "\n",
    "# Rename 'question' to 'text' if needed for tokenization\n",
    "\n",
    "if \"question\" in emo_train.column_names:\n",
    "    emo_train = emo_train.rename_column(\"question\", \"text\")\n",
    "if \"question\" in emo_test.column_names:\n",
    "    emo_test = emo_test.rename_column(\"question\", \"text\")\n",
    "print(\"After column rename:\", emo_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "da1eddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: tokenize using the 'text' column.\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "74efbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cast labels to float32 using numpy\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "e415bbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dff979cdcd04907b2808efca2684cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf4027461904851b12a6a471ef1d1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3640c16203fa4836b4b0a37b99d7973f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5a1a8e83b744cdbe42ebe13c709c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154 â€¢ Tokenized testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and cast labels for both training and testing sets (batched processing)\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"Tokenized training examples: {len(emo_train_tok)} â€¢ Tokenized testing examples: {len(emo_test_tok)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "e266af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for saving and loading models\n",
    "\n",
    "model_paths = {\n",
    "    \"t5_qa\": \"./saved_models/t5_qa\",\n",
    "    \"emotion_classifier\": \"./saved_models/emotion_classifier\",\n",
    "    \"flan_t5_response_generator\": \"./saved_models/flan_t5_response_generator\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train Emotion Classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "55885294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned emotion model...\n"
     ]
    }
   ],
   "source": [
    "# Define paths for saving and loading models\n",
    "\n",
    "emotion_model_path = SAVE_ROOT / \"emotion_classifier\"\n",
    "\n",
    "try:\n",
    "    if emotion_model_path.is_dir() and any(emotion_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned emotion model...\")\n",
    "        emo_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Fine-tuned model not found. Falling back to base model...\")\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    print(\"â¬‡Loading base Roberta model for multi-label classification...\")\n",
    "    \n",
    "    # Fallback labels if not defined yet\n",
    "    \n",
    "    if \"GO_EMOTION_LABELS\" not in globals():\n",
    "        GO_EMOTION_LABELS = [\n",
    "            'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "            'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "            'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
    "            'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "            'remorse', 'sadness', 'surprise', 'neutral'\n",
    "        ]\n",
    "\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(GO_EMOTION_LABELS)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "2aefce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "\n",
    "device = torch.device(\"cpu\" if USE_CPU else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Training device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "eeb9f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for Model Training: Use fixed max_length\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "35dfa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float conversion: ensures labels become float32 arrays.\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "e627e883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ef24b24324f0f93470e72bba8b2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e2bcc140c24f97b322f6c64800ac0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65d19634d46483b918db3056ea51d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae7e639d71548ceb6ccea6ba20a0d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization: 154 training examples; 18 testing examples.\n"
     ]
    }
   ],
   "source": [
    "# Re-tokenize the datasets using the new tokenization function\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"After tokenization: {len(emo_train_tok)} training examples; {len(emo_test_tok)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a6aac608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: SamLowe/roberta-base-go_emotions\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained emotion classification model\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,  # equals len(GO_EMOTION_LABELS)\n",
    ").to(device)\n",
    "print(\"Loaded model:\", emo_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "4d6e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metrics for Emotion Classification\n",
    "\n",
    "def compute_emo_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Compute probabilities using sigmoid on logits\n",
    "\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    # Apply threshold (0.3) to decide positive labels\n",
    "\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: only consider rows with at least one positive label\n",
    "    \n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "5690b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set Up Training Arguments\n",
    "\n",
    "# Define a root folder for saving model checkpoints if not defined already\n",
    "\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \n",
    "    # Logging & Reporting\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Training hyperâ€‘parameters\n",
    "    \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    # Evaluation and checkpointing settings\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "a9069e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_41200\\351596824.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = MultiLabelTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Custom Trainer for Multi-Label Classification\n",
    "\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "e1ea3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_41200\\747234941.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying copy of model.safetensors due to error: [WinError 5] Access is denied: 'saved_models\\\\emotion_classifier\\\\model.safetensors'\n",
      "Retrying copy of model.safetensors due to error: [WinError 5] Access is denied: 'saved_models\\\\emotion_classifier\\\\model.safetensors'\n",
      "Retrying copy of model.safetensors due to error: [WinError 5] Access is denied: 'saved_models\\\\emotion_classifier\\\\model.safetensors'\n",
      "Emotion classifier model and tokenizer saved to: saved_models\\emotion_classifier\n"
     ]
    }
   ],
   "source": [
    "# Train Roberta Emotion Classifier and Save Safely\n",
    "\n",
    "num_labels = len(emo_train_tok[0]['labels'])\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "# Training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saved_models/emotion_classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0).astype(int)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds, average=\"micro\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"micro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"micro\"),\n",
    "        \"accuracy\": accuracy_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "\n",
    "trainer_emo = Trainer(\n",
    "    model=emotion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# trainer_emo.train()\n",
    "\n",
    "# Clean up memory before saving\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Safe saving using temp dir and retry logic\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "\n",
    "    # Save model via trainer\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            trainer_emo.save_model(tmp_path)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Retrying model save due to: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Save tokenizer\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            emo_tokenizer.save_pretrained(tmp_path)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Retrying tokenizer save due to: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Final destination path\n",
    "\n",
    "    final_path = SAVE_ROOT / \"emotion_classifier\"\n",
    "    final_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move files to destination safely\n",
    "    \n",
    "    for item in tmp_path.iterdir():\n",
    "        dest = final_path / item.name\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                if dest.exists():\n",
    "                    if dest.is_file():\n",
    "                        try:\n",
    "                            dest.unlink()\n",
    "                        except PermissionError:\n",
    "                            time.sleep(1)\n",
    "                            dest.unlink()\n",
    "                    elif dest.is_dir():\n",
    "                        shutil.rmtree(dest)\n",
    "                shutil.copy2(item, dest)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Retrying copy of {item.name} due to error: {e}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "print(\"Emotion classifier model and tokenizer saved to:\", final_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "10739d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned flan-t5-large response generator...\n"
     ]
    }
   ],
   "source": [
    "# Load T5 for Response Generation\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-large\"\n",
    "SAVE_DIR = SAVE_ROOT / \"flan_t5_response_generator\"\n",
    "\n",
    "try:\n",
    "    if SAVE_DIR.is_dir() and any(SAVE_DIR.iterdir()):\n",
    "        print(\"Loading previously fine-tuned flan-t5-large response generator...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(SAVE_DIR).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base FLAN-T5-XL model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "0f801b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target.\n",
    "\n",
    "def build_t5_pairs(example):\n",
    "\n",
    "    # Retrieve the question from either \"question\" or \"text\" columns,\n",
    "    # and the corresponding answer from either \"answer\" or \"response\" columns.\n",
    "    \n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"respond: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "77cd18d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de9a862b80f4c6e970f06500a8937a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d3400fe8c94298a62f656045f30c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the original training and testing datasets to build T5 pairs.\n",
    "# We're using the original QA datasets (train_dataset and test_dataset) from our data-loading cell.\n",
    "\n",
    "resp_train = train_dataset.map(build_t5_pairs)\n",
    "resp_test  = test_dataset.map(build_t5_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "be25e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer and model.\n",
    "\n",
    "t5_resp_model_name = \"google/flan-t5-large\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3234e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "\n",
    "def t5_tokenize(batch):\n",
    "    inputs = tokenizer_t5(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "fe4ebc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab29ac69c2ce446cbde4a5e151079303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bd4de14a09464e9bd69ea318da0525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154; Tokenized testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "# Set datasets' format to output PyTorch tensors.\n",
    "\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized training examples: {len(resp_train_tok)}; Tokenized testing examples: {len(resp_test_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "df88f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned FLAN-T5 model...\n"
     ]
    }
   ],
   "source": [
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata\n",
    "\n",
    "flan_resp_path = Path(model_paths[\"flan_t5_response_generator\"])\n",
    "\n",
    "try:\n",
    "    if flan_resp_path.is_dir() and any(flan_resp_path.iterdir()):\n",
    "        print(\"Loading fine-tuned FLAN-T5 model...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(flan_resp_path).to(device)\n",
    "        resp_tokenizer = AutoTokenizer.from_pretrained(flan_resp_path)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base flan-t5-large model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "    resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "2b2af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.where(labels != -100, labels, resp_tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = resp_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = resp_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "0d8824d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_41200\\1285048825.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  resp_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying copy of pytorch_model.bin due to error: [WinError 5] Access is denied: 'saved_models\\\\flan_t5_response_generator\\\\pytorch_model.bin'\n",
      "Retrying copy of pytorch_model.bin due to error: [WinError 5] Access is denied: 'saved_models\\\\flan_t5_response_generator\\\\pytorch_model.bin'\n",
      "Retrying copy of pytorch_model.bin due to error: [WinError 5] Access is denied: 'saved_models\\\\flan_t5_response_generator\\\\pytorch_model.bin'\n",
      "FLAN-T5 response generator model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set up the Seq2Seq training arguments.\n",
    "\n",
    "resp_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/flan_t5_response_generator\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    save_safetensors=False,\n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "# Assert that tokenized response data exists\n",
    "\n",
    "assert 'resp_train_tok' in globals() and 'resp_test_tok' in globals(), \"Tokenized response data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "\n",
    "resp_trainer = Seq2SeqTrainer(\n",
    "    model=resp_model,  \n",
    "    args=resp_training_args,\n",
    "    train_dataset=resp_train_tok,  \n",
    "    eval_dataset=resp_test_tok,    \n",
    "    tokenizer=resp_tokenizer,      \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=resp_tokenizer, model=resp_model),\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Begin training\n",
    "\n",
    "# resp_trainer.train()\n",
    "\n",
    "# Optional: Free up memory (especially useful on GPU systems)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save model and tokenizer safely using a temp directory\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "\n",
    "    # Save model and tokenizer to temp directory\n",
    "\n",
    "    resp_model.save_pretrained(tmp_path, safe_serialization=False)\n",
    "    resp_tokenizer.save_pretrained(tmp_path)\n",
    "\n",
    "    # Define final destination\n",
    "\n",
    "    final_path = SAVE_ROOT / \"flan_t5_response_generator\"\n",
    "    final_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Move files, overwriting if needed (with retry logic for Windows locks)\n",
    "    \n",
    "    for item in tmp_path.iterdir():\n",
    "        dest = final_path / item.name\n",
    "        for _ in range(3):  # Try up to 3 times\n",
    "            try:\n",
    "                if dest.exists():\n",
    "                    if dest.is_file():\n",
    "                        try:\n",
    "                            dest.unlink()\n",
    "                        except PermissionError:\n",
    "                            time.sleep(1)\n",
    "                            dest.unlink()\n",
    "                    elif dest.is_dir():\n",
    "                        shutil.rmtree(dest)\n",
    "                shutil.copy2(item, dest) \n",
    "                break  # success\n",
    "            except Exception as e:\n",
    "                print(f\"Retrying copy of {item.name} due to error: {e}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "print(\"FLAN-T5 response generator model and tokenizer saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d398f38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded response model: flan_t5_response_generator\n"
     ]
    }
   ],
   "source": [
    "# Load the saved FLAN-T5 response generator model from disk\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/flan_t5_response_generator\").to(device)\n",
    "print(\"Loaded response model: flan_t5_response_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## Train T5 for Questionâ€‘Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "39ac8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned T5 QA model from: saved_models\\t5_qa\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned or base T5 QA model\n",
    "\n",
    "qa_model_path = Path(model_paths[\"t5_qa\"])\n",
    "\n",
    "try:\n",
    "    if qa_model_path.is_dir() and any(qa_model_path.iterdir()):\n",
    "        print(\"Loading fine-tuned T5 QA model from:\", qa_model_path)\n",
    "        t5_qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(device)\n",
    "        t5_qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No fine-tuned QA model found.\")\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    print(\"Loading base T5 model (google/flan-t5-base)...\")\n",
    "    t5_qa_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "    t5_qa_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e6b533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build QA-style input/target pairs for T5 (question â†’ answer)\n",
    "\n",
    "def build_qa_pairs(example):\n",
    "    question = example.get(\"question\", \"\") or example.get(\"text\", \"\")\n",
    "    answer = example.get(\"answer\", \"\") or example.get(\"response\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"input_text\": f\"question: {question.strip()}\",\n",
    "        \"target_text\": answer.strip()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "3871fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fresh T5 QA model and tokenizer (not from disk â€” training from scratch)\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "460a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input/target pairs for T5 (question â†’ answer)\n",
    "\n",
    "def qa_tokenize(batch):\n",
    "    context = batch.get(\"context\", [\"\"] * len(batch[\"question\"]))\n",
    "\n",
    "    # Tokenize inputs\n",
    "\n",
    "    inputs = qa_tokenizer(\n",
    "        batch[\"question\"],\n",
    "        context,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "\n",
    "    labels = qa_tokenizer(\n",
    "        batch[\"answer\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # IMPORTANT: convert list-of-lists into a tensor-friendly format\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "ba89503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e4e40168944d9babdeb31ee57a3e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467fdb95f084d538fa61312c49c42d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecc40d259d846019fe5bfe0b8776783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6b096900784bbc8badb9d58d1d566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build training and testing datasets\n",
    "\n",
    "qa_train = train_dataset.map(build_qa_pairs)\n",
    "qa_test  = test_dataset.map(build_qa_pairs)\n",
    "\n",
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "f7e30f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.where(labels != -100, labels, t5_qa_tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = t5_qa_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = t5_qa_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "7c4d1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_41200\\2249035596.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  qa_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to copy pytorch_model.bin: [WinError 5] Access is denied: 'saved_models\\\\t5_qa\\\\pytorch_model.bin'\n",
      "T5 QA model and tokenizer saved safely.\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "\n",
    "qa_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/t5_qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    save_safetensors=False,  \n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "# Assert to ensure tokenized data is defined\n",
    "\n",
    "assert 'qa_train_tok' in globals() and 'qa_test_tok' in globals(), \"Tokenized QA data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "\n",
    "qa_trainer = Seq2SeqTrainer(\n",
    "    model=t5_qa_model,\n",
    "    args=qa_training_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=t5_qa_tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_qa_tokenizer, model=t5_qa_model),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the QA model\n",
    "\n",
    "# qa_trainer.train()\n",
    "\n",
    "# Optional: Free up memory (especially useful on GPU systems)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save model and tokenizer safely using a temp directory\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "\n",
    "    # Save model + tokenizer to temp dir\n",
    "\n",
    "    qa_trainer.save_model(tmp_path)\n",
    "    t5_qa_tokenizer.save_pretrained(tmp_path)\n",
    "\n",
    "    # Final destination\n",
    "\n",
    "    final_path = SAVE_ROOT / \"t5_qa\"\n",
    "    final_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move files: overwrite by removing first if necessary\n",
    "\n",
    "    for item in tmp_path.iterdir():\n",
    "        dest = final_path / item.name\n",
    "        try:\n",
    "            if dest.exists():\n",
    "                if dest.is_file():\n",
    "                    dest.unlink()\n",
    "                elif dest.is_dir():\n",
    "                    shutil.rmtree(dest)\n",
    "            shutil.copy2(item, dest)  \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy {item.name}: {e}\")\n",
    "\n",
    "print(\"T5 QA model and tokenizer saved safely.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "1fd0f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned T5 QA model...\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned T5 QA model, or fall back to base if not found\n",
    "\n",
    "qa_model_path = SAVE_ROOT / \"t5_qa\"\n",
    "\n",
    "try:\n",
    "    if qa_model_path.is_dir() and any(qa_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned T5 QA model...\")\n",
    "        qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"T5 QA model directory is empty or missing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Falling back to base T5 model (google/flan-t5-large)...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a35b0",
   "metadata": {},
   "source": [
    "## Model Deployment Setup with Combined Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f55db508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined model metadata to: saved_models\\combined_model_metadata.pt\n"
     ]
    }
   ],
   "source": [
    "# Save Combined Model Metadata\n",
    "\n",
    "metadata = {\n",
    "    \"emotion_classifier\": str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \"flan_t5_response_generator\": str(SAVE_ROOT / \"flan_t5_response_generator\"), \n",
    "    \"t5_qa\": str(SAVE_ROOT / \"t5_qa\")\n",
    "}\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "torch.save(metadata, metadata_path)\n",
    "print(\"Saved combined model metadata to:\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "68f1a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 'flan_t5_response_generator': 'saved_models\\\\flan_t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n",
      "All models loaded from metadata.\n"
     ]
    }
   ],
   "source": [
    "# Load Models Using Combined Metadata\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")\n",
    "\n",
    "# Load Emotion Classification Model & Tokenizer from metadata\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "\n",
    "print(\"All models loaded from metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8533473",
   "metadata": {},
   "source": [
    "## Unified Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1befa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 'flan_t5_response_generator': 'saved_models\\\\flan_t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n"
     ]
    }
   ],
   "source": [
    "# Load Fineâ€‘Tuned Models Using Combined Metadata. Define the path to the combined metadata file.\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device,)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "7e2e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Emotion Classification Model & Tokenizer from metadata.\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "emo_model.eval()  \n",
    "\n",
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata.\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"flan_t5_response_generator\"]).to(device)\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"flan_t5_response_generator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "10565767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels. These labels match those used in our emotion annotation step.\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "5cf0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, define a subset for emotion-based routing.\n",
    "\n",
    "emotion_router_labels = {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "f4828899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fineâ€‘tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "34836f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "6d15c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Chatbot Pipeline Class\n",
    "\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []\n",
    "\n",
    "        # Load models (assumed already loaded globally from metadata)\n",
    "\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    def __call__(self, text, max_length=64):\n",
    "        self.chat_history.append((\"User\", text))\n",
    "\n",
    "        # Detect emotions\n",
    "\n",
    "        emotions = detect_emotions(text)\n",
    "\n",
    "        # Decide which model to use\n",
    "\n",
    "        if \"?\" in text:\n",
    "            model, tokenizer = self.qa_model, qa_tokenizer\n",
    "        else:\n",
    "            model, tokenizer = self.resp_model, resp_tokenizer\n",
    "\n",
    "        try:\n",
    "            # Prepare input\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate output\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(**inputs, max_length=max_length)\n",
    "            reply = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            reply = \"Sorry, something went wrong.\"\n",
    "\n",
    "        self.chat_history.append((\"Bot\", reply))\n",
    "\n",
    "        return {\n",
    "            \"Detected Emotions\": emotions,\n",
    "            \"Response\": reply,\n",
    "            \"History\": self.chat_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a8306fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Chatbot Pipeline Class\n",
    "\n",
    "def generate_chatbot_response(user_text, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    emotions = detect_emotions(user_text)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(\n",
    "            user_text,\n",
    "            language=language,\n",
    "            history=history if use_history else None,\n",
    "            emotions=emotions\n",
    "        )\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_text\n",
    "        inputs = t5_qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, t5_qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_text}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "0233cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the chatbot pipeline.\n",
    "\n",
    "chatbot = MentalHealthChatbotPipeline(labels=EMOTION_LABELS, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95721f",
   "metadata": {},
   "source": [
    "## Gradio Interface for the Mental Health Chatbot (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "c0299535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 'flan_t5_response_generator': 'saved_models\\\\flan_t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n"
     ]
    }
   ],
   "source": [
    "# Load Fine-Tuned Models Using Combined Metadata\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "55e0e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 785,509,376 || trainable%: 0.3004\n"
     ]
    }
   ],
   "source": [
    "# Load Response Generation Model & Tokenizer\n",
    "\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"], \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "resp_model = get_peft_model(base_model, lora_config)\n",
    "resp_model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "17593d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete list of 28 emotion labels (as in the GoEmotions baseline)\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "7e1a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fine-Tuned Emotion Classifier\n",
    "\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "\n",
    "# Define a subset of emotions for routing decisions (if needed)\n",
    "\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "a789f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "35881b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fineâ€‘tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # Safety check: trim probabilities if there are more than expected.\n",
    "    \n",
    "    if len(probs) > len(EMOTION_LABELS):\n",
    "        print(f\"Warning: Received {len(probs)} probabilities; expected {len(EMOTION_LABELS)}. Trimming extra values.\")\n",
    "        probs = probs[:len(EMOTION_LABELS)]\n",
    "    \n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "a6981e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatbot_response(user_input, language=\"English\", use_history=False, history=None, route_by_emotion=False, persist=False):\n",
    "    history = history or []\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        \n",
    "        # Add emotion and history into the prompt\n",
    "\n",
    "        emotion_note = f\"The user seems to feel {', '.join(emotions)}. \" if emotions and emotions != [\"neutral\"] else \"\"\n",
    "        if use_history and history:\n",
    "            conversation = \"\\n\".join(history + [user_input])\n",
    "            prompt = f\"respond: {emotion_note}The conversation so far:\\n{conversation}\"\n",
    "        else:\n",
    "            prompt = f\"respond: {emotion_note}{user_input}\"\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=100,\n",
    "            min_length=20,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, emotions, full_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "1a503989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a chatbot response (emotions will be detected inside the function)\n",
    "\n",
    "def chatbot_interface(user_input):\n",
    "    \n",
    "    response, emotions, _ = generate_chatbot_response(\n",
    "        user_input=user_input,\n",
    "        language=\"English\",\n",
    "        use_history=False,\n",
    "        history=[],\n",
    "        route_by_emotion=True,\n",
    "        persist=False\n",
    "    )\n",
    "\n",
    "    return f\"Detected Emotions: {', '.join(emotions)}\\n\\nBot: {response}\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=gr.Textbox(label=\"User Input\"),\n",
    "    outputs=gr.Textbox(label=\"Chatbot Response\"),\n",
    "    title=\"ðŸ§  Happy Brain Mental Health Chatbot\",\n",
    "    description=\"Emotion-aware supportive chatbot (text only)\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf78da",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "cbb02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Emotion Classification\n",
    "\n",
    "def evaluate_emotion_classifier(model, tokenizer, dataset, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the emotion classifier over the provided dataset.\n",
    "    Computes micro-averaged F1, Precision, Recall, and Subset Accuracy.\n",
    "    Assumes that dataset is formatted with columns \"input_ids\", \"attention_mask\", \"labels\"\n",
    "    and that labels is a multi-label binary vector.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create a DataLoader for batch processing (if dataset is not huge, you can loop through it directly)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # Move inputs and labels to device\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        # Apply sigmoid for multi-label classification and threshold at 0.3\n",
    "\n",
    "        preds = (torch.sigmoid(logits) > 0.3).cpu().numpy().astype(int)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Compute micro-averaged metrics\n",
    "    \n",
    "    micro_f1 = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_precision = precision_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_recall = recall_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    subset_acc = accuracy_score(all_labels, all_preds)  # subset accuracy is strict\n",
    "    \n",
    "    return {\n",
    "        \"Emotion Classifier Micro-F1\": micro_f1,\n",
    "        \"Emotion Classifier Micro-Precision\": micro_precision,\n",
    "        \"Emotion Classifier Micro-Recall\": micro_recall,\n",
    "        \"Emotion Classifier Subset Accuracy\": subset_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "05735854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Emotion Classifier...\n",
      "Emotion Classifier Micro-F1: 0.2000\n",
      "Emotion Classifier Micro-Precision: 0.1562\n",
      "Emotion Classifier Micro-Recall: 0.2778\n",
      "Emotion Classifier Subset Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"Evaluating Emotion Classifier...\")\n",
    "emo_metrics = evaluate_emotion_classifier(emo_model, emo_tokenizer, emo_test_tok)\n",
    "for metric, value in emo_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "7abddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Generation Models (Response Generation and QA). We already defined a compute_resp_metrics function in the training cells.\n",
    "# Here, we define a helper to compute additional perplexity based on the evaluation loss.\n",
    "\n",
    "def evaluate_generation_model(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Uses the Seq2SeqTrainer to compute evaluation metrics over the given test dataset.\n",
    "    Adds perplexity (exp(eval_loss)) to the standard metrics.\n",
    "    \"\"\"\n",
    "    # Predict returns a dictionary with metrics: eval_loss, and any metrics computed in compute_metrics.\n",
    "\n",
    "    result = trainer.predict(test_dataset)\n",
    "    eval_loss = result.metrics.get(\"eval_loss\")\n",
    "    \n",
    "    # Compute perplexity if loss is available. (If eval_loss is zero or not available, perplexity is undefined.)\n",
    "    \n",
    "    if eval_loss is not None and eval_loss > 0:\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    else:\n",
    "        perplexity = float(\"inf\")\n",
    "    \n",
    "    # Add perplexity to the metrics dictionary.\n",
    "    \n",
    "    result.metrics[\"perplexity\"] = perplexity\n",
    "    return result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "98aa20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating T5 Response Generation Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Response Generation test_loss: 14.2457\n",
      "T5 Response Generation test_model_preparation_time: 0.0240\n",
      "T5 Response Generation test_rougeL: 0.3537\n",
      "T5 Response Generation test_runtime: 10.7264\n",
      "T5 Response Generation test_samples_per_second: 1.6780\n",
      "T5 Response Generation test_steps_per_second: 0.4660\n",
      "T5 Response Generation perplexity: inf\n",
      "\n",
      "Evaluating T5 QA Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 QA test_loss: 2.7052\n",
      "T5 QA test_model_preparation_time: 0.0080\n",
      "T5 QA test_rougeL: 0.4148\n",
      "T5 QA test_runtime: 3.7120\n",
      "T5 QA test_samples_per_second: 4.8490\n",
      "T5 QA test_steps_per_second: 1.3470\n",
      "T5 QA perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"\\nEvaluating T5 Response Generation Model...\")\n",
    "resp_metrics = evaluate_generation_model(resp_trainer, resp_test_tok)\n",
    "for metric, value in resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating T5 QA Model...\")\n",
    "qa_metrics = evaluate_generation_model(qa_trainer, qa_test_tok)\n",
    "for metric, value in qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "d853ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def compute_generation_metrics(trainer, dataset, tokenizer):\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "    \n",
    "    results = trainer.predict(dataset)\n",
    "    predictions, labels = results.predictions, results.label_ids\n",
    "\n",
    "    # If predictions come as a tuple (e.g., logits), extract the actual token IDs\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Convert logits to token IDs if needed (argmax across vocab dim)\n",
    "    \n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
