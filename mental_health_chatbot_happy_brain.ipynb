{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Mentalâ€‘Health Chatbot: Happy Brain\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "This project builds an **emotion-aware mental health chatbot** designed to provide both empathetic support and factual information. The chatbot intelligently routes user input through either a **response generator** or a **question-answering system**, based on both emotional context and input type.\n",
    "\n",
    "The system integrates:\n",
    "- **Emotion Classification:** A fine-tuned `SamLowe/roberta-base-go_emotions` model to detect multi-label emotions in user messages.\n",
    "- **Text Generation:**\n",
    "  - **Supportive Response Generation:** A fine-tuned `google/flan-t5-large` model trained to provide compassionate, therapist-style replies.\n",
    "  - **Question Answering (QA):** A separate `google/flan-t5-base` model trained to handle factual queries in a Q&A format.\n",
    "- **Routing Mechanism:** Uses emotion detection and the presence of a question to route the input to the appropriate model.\n",
    "- **Deployment:** The chatbot is deployed using a **Streamlit interface** that supports typed input, emotion display, and chat history.\n",
    "\n",
    "## Notebook Structure and Sections\n",
    "\n",
    "1. **Library Imports & Environment Setup**  \n",
    "   - Configures PyTorch, Hugging Face, and device management (including CPU-only fallback).\n",
    "   - Handles warnings, seed setting, and save directory creation.\n",
    "\n",
    "2. **Data Loading & Preprocessing**  \n",
    "   - Loads and normalizes multiple mental health datasets.\n",
    "   - Cleans and maps data into standardized \"question\"/\"answer\" pairs.\n",
    "   - Splits data into training and testing sets.\n",
    "\n",
    "3. **Multi-Label Emotion Annotation & Custom Trainer Setup**  \n",
    "   - Converts labels into multi-hot vectors using `MultiLabelBinarizer`.\n",
    "   - Defines a custom data collator and Trainer for multi-label classification.\n",
    "   - Tokenizes input text for Roberta model.\n",
    "\n",
    "4. **Model Training: Emotion Classification**  \n",
    "   - Fine-tunes the `roberta-base-go_emotions` model.\n",
    "   - Uses micro F1, precision, recall, and subset accuracy for evaluation.\n",
    "   - Saves the trained model to `./saved_models/emotion_classifier`.\n",
    "\n",
    "5. **Model Training: FLAN-T5 for Response Generation**  \n",
    "   - Constructs input/target pairs from chat-like responses.\n",
    "   - Trains `flan-t5-large` using ROUGE as the evaluation metric.\n",
    "   - Caches results and disables safetensors to prevent serialization issues on Windows.\n",
    "   - Saves to `./saved_models/flan_t5_response_generator`.\n",
    "\n",
    "6. **Model Training: FLAN-T5 for Question Answering**  \n",
    "   - Converts QA pairs to \"question: <text>\" format.\n",
    "   - Fine-tunes `flan-t5-base` to generate precise, factual answers.\n",
    "   - Uses custom evaluation logic to handle label padding and decoding.\n",
    "   - Saves to `./saved_models/t5_qa`.\n",
    "\n",
    "7. **Model Deployment Setup with Combined Metadata**  \n",
    "   - Writes a `combined_model_metadata.pt` file containing paths to the fine-tuned models.\n",
    "   - Enables clean and modular model loading in deployment.\n",
    "\n",
    "8. **Unified Pipeline for Streamlit Interface**  \n",
    "   - Loads all three models and tokenizers from the metadata.\n",
    "   - Defines emotion-based routing logic, input formatting, and history-aware generation.\n",
    "   - Implements a unified `MentalHealthChatbotPipeline` for text input.\n",
    "   - Designed for integration into a **Streamlit app**, supporting input, emotion detection, output display, and session-based conversation history.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook delivers a **production-ready mental health chatbot pipeline**, with modular training, model saving, and dynamic routing. All components are saved and loaded via metadata, supporting flexible deployment options. The final chatbot uses both emotion detection and semantic intent to respond appropriatelyâ€”whether with therapeutic support or factual answers.\n",
    "\n",
    "The full system is trained, validated, and ready for deployment using a **Streamlit interface**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Environment Setup\n",
    "# ================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force CPU use\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ================================\n",
    "# Standard Library Imports\n",
    "# ================================\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tempfile\n",
    "import datetime\n",
    "import pprint  # Pretty-printing\n",
    "\n",
    "# ================================\n",
    "# Scientific & Data Libraries\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ================================\n",
    "# Audio Processing\n",
    "# ================================\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ================================\n",
    "# NLP & Transformers (Hugging Face)\n",
    "# ================================\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import streamlit as st\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Transformers - Tokenizers & Models\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "# Transformers - Training Tools\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSequenceClassification,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "# Transformers - Logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# ================================\n",
    "# PEFT / LoRA\n",
    "# ================================\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea550a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cpu\n",
      "FORCED device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "\n",
    "USE_CPU = True  # Set to True to force CPU mode\n",
    "device = torch.device('cpu' if USE_CPU else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Device in use:', device)\n",
    "\n",
    "# FORCE CPU for the entire session\n",
    "\n",
    "USE_CPU = True\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"FORCED device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Save directory setup\n",
    "\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"flan_t5_response_generator\", \"t5_qa\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b54c6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "\n",
    "DATASETS = {\n",
    "    \"ds1\": (True,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False, \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False, \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False, \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False, \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b472611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner that auto-maps columns to 'question' / 'answer'\n",
    "\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # Common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # If provided cols exist, rename them to standard names\n",
    "    \n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "    # Try to map 'context' -> 'question' if needed\n",
    "\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    # Verify if necessary columns exist\n",
    "\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    # Retain only required columns, drop missing values and duplicates\n",
    "\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    \n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565109d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'ds1' from ./data/ds1_transformed_mental_health_chatbot_dataset.csv ...\n",
      "Loaded 172 examples from 'ds1'.\n",
      "Skipping dataset 'ds2' as its toggle is off.\n",
      "Skipping dataset 'ds3' as its toggle is off.\n",
      "Skipping dataset 'ds4' as its toggle is off.\n",
      "Skipping dataset 'ds5' as its toggle is off.\n",
      "Skipping dataset 'ds6' as its toggle is off.\n"
     ]
    }
   ],
   "source": [
    "# Load enabled datasets and create a unified dataset\n",
    "\n",
    "datasets = []\n",
    "for key, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if enabled:\n",
    "        print(f\"Loading dataset '{key}' from {path} ...\")\n",
    "        ds = load_and_clean(path, q_col, a_col)\n",
    "        print(f\"Loaded {len(ds)} examples from '{key}'.\")\n",
    "        datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Skipping dataset '{key}' as its toggle is off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd09483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined dataset contains 172 examples.\n",
      "Training set: 154 examples, Testing set: 18 examples.\n"
     ]
    }
   ],
   "source": [
    "# Create unified dataset\n",
    "\n",
    "if datasets:\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    print(f\"\\nCombined dataset contains {len(combined_dataset)} examples.\")\n",
    "else:\n",
    "    raise ValueError(\"No datasets enabled. Please enable at least one dataset in DATASETS.\")\n",
    "\n",
    "# Shuffle and split into training and testing datasets (e.g., 90% train, 10% test)\n",
    "\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "print(f\"Training set: {len(train_dataset)} examples, Testing set: {len(test_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228172",
   "metadata": {},
   "source": [
    "### Multi-Label Emotion Annotation & Custom Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8030b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed if not already defined\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Data Collator: casts labels to float32\n",
    "\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "\n",
    "    # Uncomment next line to print label info during debugging\n",
    "    # print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3af2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for Multi-Label Classification\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that computes loss using binary crossâ€‘entropy with logits.\n",
    "    This ensures multiâ€‘label targets (e.g., emotions) are correctly processed.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # If labels' shape does not match logits, reshape to match\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0db1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback: Print training progress (optional)\n",
    "\n",
    "class StepPrinter(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Trainer callback that prints step-wise loss and evaluation metrics\n",
    "    while keeping the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} â€¢ loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" â€¢ metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" â€¢ eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbe30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels (28 total: 27 + neutral)\n",
    "\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d31ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate each example with multi-label emotion annotations.\n",
    "# For this demo, we simulate emotion annotations: if an example has no emotion,\n",
    "# we default to \"neutral\". Replace this with your real emotion annotations if available.\n",
    "\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no explicit emotion annotations, default to [\"neutral\"]\n",
    "\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    # Save original emotions for visibility (optional)\n",
    "\n",
    "    example[\"emotions\"] = emos\n",
    "    \n",
    "    # Create a binary label vector for each of the 28 emotions\n",
    "    \n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5a3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 154 examples â€¢ Test dataset: 18 examples\n"
     ]
    }
   ],
   "source": [
    "# Helper to retrieve input text from an example (for debugging)\n",
    "\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\"\n",
    "\n",
    "# (Reâ€‘)load datasets for emotion annotation if not already loaded.\n",
    "\n",
    "if 'train_dataset' not in globals() or 'test_dataset' not in globals():\n",
    "    print(\"Reloading datasets for emotion annotation ...\")\n",
    "    datasets_list = []\n",
    "    for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "        if not enabled:\n",
    "            continue\n",
    "        ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "        datasets_list.append(ds)\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "        fallback_data = {\n",
    "            \"text\": [\n",
    "                \"How are you?\",\n",
    "                \"I feel really down today.\",\n",
    "                \"I'm so happy with my progress!\",\n",
    "                \"Why does nobody understand me?\",\n",
    "                \"I'm feeling anxious about school.\",\n",
    "                \"Life is good lately.\",\n",
    "                \"Sometimes I just want to cry.\",\n",
    "                \"Everything is falling apart.\",\n",
    "                \"Iâ€™m grateful for my therapist.\",\n",
    "                \"Can someone please just listen?\"\n",
    "            ]\n",
    "        }\n",
    "        ds = Dataset.from_dict(fallback_data)\n",
    "        datasets_list.append(ds)\n",
    "    full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "    full_ds = full_ds.shuffle(seed=SEED)\n",
    "    split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} examples â€¢ Test dataset: {len(test_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f137d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fc693dc65a4b239e7c580dd568cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2690df598749e891501cfaafcac8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample annotation:\n",
      "How does someone acquire a mental illness? -> ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# Map emotion annotations onto training and testing datasets\n",
    "\n",
    "emo_train = train_dataset.map(annotate_emotions)\n",
    "emo_test  = test_dataset.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample annotation:\")\n",
    "print(get_input_text(emo_train[0]), \"->\", emo_train[0][\"emotions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59fbdc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007c9782e8c74d11a30932845d7f9995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36e6326bff545ebbea37e0a8e600fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with problematic labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter out any examples without at least one positive label (shouldn't happen after defaulting to neutral)\n",
    "\n",
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n",
    "\n",
    "# Check for any problematic labels\n",
    "\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "print(\"Number of examples with problematic labels:\", len(bad_labels))\n",
    "if bad_labels:\n",
    "    print(\"Example with problematic labels:\", bad_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b16533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before column rename: ['question', 'answer', 'emotions', 'labels']\n",
      "After column rename: ['text', 'answer', 'emotions', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for emotion classification.\n",
    "# We'll be using the 'SamLowe/roberta-base-go_emotions' tokenizer.\n",
    "\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Print columns before renaming for clarity\n",
    "\n",
    "print(\"Before column rename:\", emo_train.column_names)\n",
    "\n",
    "# Rename 'question' to 'text' if needed for tokenization\n",
    "\n",
    "if \"question\" in emo_train.column_names:\n",
    "    emo_train = emo_train.rename_column(\"question\", \"text\")\n",
    "if \"question\" in emo_test.column_names:\n",
    "    emo_test = emo_test.rename_column(\"question\", \"text\")\n",
    "print(\"After column rename:\", emo_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1eddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: tokenize using the 'text' column.\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74efbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cast labels to float32 using numpy\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e415bbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2328ce0d1df45d390e2872539e01f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac2beadb53d478ea3ac73548974b15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c199feaaf74fefaf94054ab3b84202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e2657594ad4ba386198dc38574105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154 â€¢ Tokenized testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and cast labels for both training and testing sets (batched processing)\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"Tokenized training examples: {len(emo_train_tok)} â€¢ Tokenized testing examples: {len(emo_test_tok)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e266af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for saving and loading models\n",
    "\n",
    "model_paths = {\n",
    "    \"t5_qa\": \"./saved_models/t5_qa\",\n",
    "    \"emotion_classifier\": \"./saved_models/emotion_classifier\",\n",
    "    \"flan_t5_response_generator\": \"./saved_models/flan_t5_response_generator\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train Emotion Classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55885294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned emotion model...\n"
     ]
    }
   ],
   "source": [
    "# Define paths for saving and loading models\n",
    "\n",
    "emotion_model_path = SAVE_ROOT / \"emotion_classifier\"\n",
    "\n",
    "try:\n",
    "    if emotion_model_path.is_dir() and any(emotion_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned emotion model...\")\n",
    "        emo_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Fine-tuned model not found. Falling back to base model...\")\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    print(\"â¬‡Loading base Roberta model for multi-label classification...\")\n",
    "    \n",
    "    # Fallback labels if not defined yet\n",
    "    \n",
    "    if \"GO_EMOTION_LABELS\" not in globals():\n",
    "        GO_EMOTION_LABELS = [\n",
    "            'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "            'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "            'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
    "            'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "            'remorse', 'sadness', 'surprise', 'neutral'\n",
    "        ]\n",
    "\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(GO_EMOTION_LABELS)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aefce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "\n",
    "device = torch.device(\"cpu\" if USE_CPU else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Training device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb9f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for Model Training: Use fixed max_length\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35dfa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float conversion: ensures labels become float32 arrays.\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e627e883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7715dfaa0844af186b3e89c93013e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a8d0b10e684e2f818fb604d4609360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3d63b92ab246849982b6387505106a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8991b8bbfbd244ff938d696733662835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization: 154 training examples; 18 testing examples.\n"
     ]
    }
   ],
   "source": [
    "# Re-tokenize the datasets using the new tokenization function\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"After tokenization: {len(emo_train_tok)} training examples; {len(emo_test_tok)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6aac608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: SamLowe/roberta-base-go_emotions\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained emotion classification model\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,  # equals len(GO_EMOTION_LABELS)\n",
    ").to(device)\n",
    "print(\"Loaded model:\", emo_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d6e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metrics for Emotion Classification\n",
    "\n",
    "def compute_emo_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Compute probabilities using sigmoid on logits\n",
    "\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    # Apply threshold (0.3) to decide positive labels\n",
    "\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: only consider rows with at least one positive label\n",
    "    \n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5690b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set Up Training Arguments\n",
    "\n",
    "# Define a root folder for saving model checkpoints if not defined already\n",
    "\n",
    "from pathlib import Path\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \n",
    "    # Logging & Reporting\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Training hyperâ€‘parameters\n",
    "    \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    \n",
    "    # Evaluation and checkpointing settings\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9069e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_21512\\351596824.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiLabelTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = MultiLabelTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Custom Trainer for Multi-Label Classification\n",
    "\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f991cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_21512\\1083929981.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.373584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.274940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.251461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Roberta Emotion Classifier\n",
    "\n",
    "num_labels = len(emo_train_tok[0]['labels'])\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saved_models/emotion_classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0).astype(int)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds, average=\"micro\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"micro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"micro\"),\n",
    "        \"accuracy\": accuracy_score(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=emotion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Comment out the next line to disable training.\n",
    "\n",
    "trainer.train()\n",
    "emotion_model.save_pretrained(\"./saved_models/emotion_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1ea3620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 01:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 â€¢ loss 0.0329\n",
      "Step     20 â€¢ loss 0.0036\n",
      "Step     30 â€¢ loss 0.0033\n",
      "Epoch 1/1 â€¢ eval_loss 0.0025\n",
      "Emotion classifier model and tokenizer saved to saved_models\\emotion_classifier\n"
     ]
    }
   ],
   "source": [
    "# Start Training (uncomment the next line to train)\n",
    "\n",
    "trainer_emo.train()\n",
    "\n",
    "# Save the Best Model and Tokenizer. Create the directory structure if it doesn't exist\n",
    "\n",
    "SAVE_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "(SAVE_ROOT / \"emotion_classifier\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "emo_model.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "emo_tokenizer.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "\n",
    "# Also save the trainer's final model state\n",
    "\n",
    "trainer_emo.save_model()\n",
    "print(\"Emotion classifier model and tokenizer saved to\", SAVE_ROOT / \"emotion_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10739d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned flan-t5-large response generator...\n"
     ]
    }
   ],
   "source": [
    "# Load T5 for Response Generation\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-large\"\n",
    "SAVE_DIR = SAVE_ROOT / \"flan_t5_response_generator\"\n",
    "\n",
    "try:\n",
    "    if SAVE_DIR.is_dir() and any(SAVE_DIR.iterdir()):\n",
    "        print(\"Loading previously fine-tuned flan-t5-large response generator...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(SAVE_DIR).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base FLAN-T5-XL model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f801b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target.\n",
    "\n",
    "def build_t5_pairs(example):\n",
    "\n",
    "    # Retrieve the question from either \"question\" or \"text\" columns,\n",
    "    # and the corresponding answer from either \"answer\" or \"response\" columns.\n",
    "    \n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"respond: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77cd18d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc5d55a1c994bbc9b0fbdc85a74e438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cb405da3cd44db93969c0d3ed5bcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the original training and testing datasets to build T5 pairs.\n",
    "# We're using the original QA datasets (train_dataset and test_dataset) from our data-loading cell.\n",
    "\n",
    "resp_train = train_dataset.map(build_t5_pairs)\n",
    "resp_test  = test_dataset.map(build_t5_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be25e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer and model.\n",
    "\n",
    "t5_resp_model_name = \"google/flan-t5-large\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3234e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "\n",
    "def t5_tokenize(batch):\n",
    "    inputs = tokenizer_t5(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe4ebc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8ad96041b64ba9a6711d52cfb5b45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e795d1c73447269471bbbe71c9e4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154; Tokenized testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "# Set datasets' format to output PyTorch tensors.\n",
    "\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized training examples: {len(resp_train_tok)}; Tokenized testing examples: {len(resp_test_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df88f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned FLAN-T5 model...\n"
     ]
    }
   ],
   "source": [
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "flan_resp_path = Path(model_paths[\"flan_t5_response_generator\"])\n",
    "\n",
    "try:\n",
    "    if flan_resp_path.is_dir() and any(flan_resp_path.iterdir()):\n",
    "        print(\"Loading fine-tuned FLAN-T5 model...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(flan_resp_path).to(device)\n",
    "        resp_tokenizer = AutoTokenizer.from_pretrained(flan_resp_path)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base flan-t5-large model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "    resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.where(labels != -100, labels, resp_tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = resp_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = resp_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8824d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_21512\\1754106336.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  resp_trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\data\\data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 15:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.417100</td>\n",
       "      <td>3.112344</td>\n",
       "      <td>0.469081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the Seq2Seq training arguments.\n",
    "\n",
    "resp_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/flan_t5_response_generator\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    save_safetensors=False,\n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "# Assert that tokenized response data exists\n",
    "\n",
    "assert 'resp_train_tok' in globals() and 'resp_test_tok' in globals(), \"Tokenized response data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "\n",
    "resp_trainer = Seq2SeqTrainer(\n",
    "    model=resp_model,  \n",
    "    args=resp_training_args,\n",
    "    train_dataset=resp_train_tok,  \n",
    "    eval_dataset=resp_test_tok,    \n",
    "    tokenizer=resp_tokenizer,      \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=resp_tokenizer, model=resp_model),\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Begin training\n",
    "resp_trainer.train()\n",
    "\n",
    "# Save model + tokenizer (no safetensors)\n",
    "save_path = SAVE_ROOT / \"flan_t5_response_generator\"\n",
    "\n",
    "resp_tokenizer.save_pretrained(save_path)\n",
    "resp_model.save_pretrained(save_path, safe_serialization=False)\n",
    "\n",
    "print(f\"FLAN-T5 response generator model and tokenizer saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398f38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded response model: flan_t5_response_generator\n"
     ]
    }
   ],
   "source": [
    "# Load the saved FLAN-T5 response generator model from disk\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/flan_t5_response_generator\").to(device)\n",
    "print(\"Loaded response model: flan_t5_response_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## Train T5 for Questionâ€‘Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned T5 QA model from: saved_models\\t5_qa\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned or base T5 QA model\n",
    "\n",
    "qa_model_path = Path(model_paths[\"t5_qa\"])\n",
    "\n",
    "try:\n",
    "    if qa_model_path.is_dir() and any(qa_model_path.iterdir()):\n",
    "        print(\"Loading fine-tuned T5 QA model from:\", qa_model_path)\n",
    "        t5_qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(device)\n",
    "        t5_qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No fine-tuned QA model found.\")\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    print(\"Loading base T5 model (google/flan-t5-base)...\")\n",
    "    t5_qa_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "    t5_qa_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build QA-style input/target pairs for T5 (question â†’ answer)\n",
    "\n",
    "def build_qa_pairs(example):\n",
    "    question = example.get(\"question\", \"\") or example.get(\"text\", \"\")\n",
    "    answer = example.get(\"answer\", \"\") or example.get(\"response\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"input_text\": f\"question: {question.strip()}\",\n",
    "        \"target_text\": answer.strip()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fresh T5 QA model and tokenizer (not from disk â€” training from scratch)\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input/target pairs for T5 (question â†’ answer)\n",
    "\n",
    "def qa_tokenize(batch):\n",
    "    context = batch.get(\"context\", [\"\"] * len(batch[\"question\"]))\n",
    "\n",
    "    # Tokenize inputs\n",
    "\n",
    "    inputs = qa_tokenizer(\n",
    "        batch[\"question\"],\n",
    "        context,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "\n",
    "    labels = qa_tokenizer(\n",
    "        batch[\"answer\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # IMPORTANT: convert list-of-lists into a tensor-friendly format\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2099d4d202a64af08df941e144feff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f23c8227ce64e599f5fa57a54e77d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d0163f37c1494f8d0e49b2ac0749f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b8c43702a4f0eab533333c8c88d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build training and testing datasets\n",
    "\n",
    "qa_train = train_dataset.map(build_qa_pairs)\n",
    "qa_test  = test_dataset.map(build_qa_pairs)\n",
    "\n",
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e30f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.where(labels != -100, labels, t5_qa_tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = t5_qa_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = t5_qa_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_15596\\367438756.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  qa_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/39 01:52 < 00:30, 0.26 it/s, Epoch 0.77/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 40\u001b[0m\n\u001b[0;32m     28\u001b[0m qa_trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mt5_qa_model,\n\u001b[0;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mqa_training_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train the QA model\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mqa_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Optional: Free up memory (especially useful on GPU systems)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\accelerate\\optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    233\u001b[0m         group,\n\u001b[0;32m    234\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         state_steps,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[1;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\optim\\adamw.py:426\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    425\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[1;32m--> 426\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    429\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "\n",
    "qa_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/t5_qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    save_safetensors=False,  \n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "# Assert to ensure tokenized data is defined\n",
    "\n",
    "assert 'qa_train_tok' in globals() and 'qa_test_tok' in globals(), \"Tokenized QA data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "\n",
    "qa_trainer = Seq2SeqTrainer(\n",
    "    model=t5_qa_model,\n",
    "    args=qa_training_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=t5_qa_tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_qa_tokenizer, model=t5_qa_model),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the QA model\n",
    "\n",
    "qa_trainer.train()\n",
    "\n",
    "# Optional: Free up memory (especially useful on GPU systems)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save model and tokenizer using the Trainer to avoid file lock issues\n",
    "\n",
    "# qa_trainer.save_model(SAVE_ROOT / \"t5_qa\")\n",
    "# t5_qa_tokenizer.save_pretrained(SAVE_ROOT / \"t5_qa\")\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Save model and tokenizer safely using a temp directory\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "\n",
    "    # Save model + tokenizer to temp dir\n",
    "    qa_trainer.save_model(tmp_path)\n",
    "    t5_qa_tokenizer.save_pretrained(tmp_path)\n",
    "\n",
    "    # Final destination\n",
    "    final_path = SAVE_ROOT / \"t5_qa\"\n",
    "    final_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move files: overwrite by removing first if necessary\n",
    "    for item in tmp_path.iterdir():\n",
    "        dest = final_path / item.name\n",
    "        try:\n",
    "            if dest.exists():\n",
    "                if dest.is_file():\n",
    "                    dest.unlink()\n",
    "                elif dest.is_dir():\n",
    "                    shutil.rmtree(dest)\n",
    "            shutil.copy2(item, dest)  # Use copy instead of move to avoid Win errors\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy {item.name}: {e}\")\n",
    "\n",
    "print(\"T5 QA model and tokenizer saved safely.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned T5 QA model, or fall back to base if not found\n",
    "\n",
    "qa_model_path = SAVE_ROOT / \"t5_qa\"\n",
    "\n",
    "try:\n",
    "    if qa_model_path.is_dir() and any(qa_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned T5 QA model...\")\n",
    "        qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"T5 QA model directory is empty or missing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Falling back to base T5 model (google/flan-t5-large)...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a35b0",
   "metadata": {},
   "source": [
    "## Model Deployment Setup with Combined Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55db508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Combined Model Metadata\n",
    "\n",
    "metadata = {\n",
    "    \"emotion_classifier\": str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \"flan_t5_response_generator\": str(SAVE_ROOT / \"flan_t5_response_generator\"), \n",
    "    \"t5_qa\": str(SAVE_ROOT / \"t5_qa\")\n",
    "}\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "torch.save(metadata, metadata_path)\n",
    "print(\"Saved combined model metadata to:\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models Using Combined Metadata\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")\n",
    "\n",
    "# Load Emotion Classification Model & Tokenizer from metadata\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "\n",
    "print(\"All models loaded from metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8533473",
   "metadata": {},
   "source": [
    "## Unified Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Pipeline & Gradio for the Mental Health Chatbot\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "\n",
    "# Load Fineâ€‘Tuned Models Using Combined Metadata. Define the path to the combined metadata file.\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device,)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Emotion Classification Model & Tokenizer from metadata.\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "emo_model.eval()  \n",
    "\n",
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata.\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"flan_t5_response_generator\"]).to(device)\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"flan_t5_response_generator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10565767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels. These labels match those used in our emotion annotation step.\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, define a subset for emotion-based routing.\n",
    "\n",
    "emotion_router_labels = {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4828899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fineâ€‘tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34836f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Chatbot Pipeline Class\n",
    "\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []\n",
    "\n",
    "        # Load models (assumed already loaded globally from metadata)\n",
    "\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    def __call__(self, text, max_length=64):\n",
    "        self.chat_history.append((\"User\", text))\n",
    "\n",
    "        # Detect emotions\n",
    "\n",
    "        emotions = detect_emotions(text)\n",
    "\n",
    "        # Decide which model to use\n",
    "\n",
    "        if \"?\" in text:\n",
    "            model, tokenizer = self.qa_model, qa_tokenizer\n",
    "        else:\n",
    "            model, tokenizer = self.resp_model, resp_tokenizer\n",
    "\n",
    "        try:\n",
    "            # Prepare input\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate output\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(**inputs, max_length=max_length)\n",
    "            reply = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            reply = \"Sorry, something went wrong.\"\n",
    "\n",
    "        self.chat_history.append((\"Bot\", reply))\n",
    "\n",
    "        return {\n",
    "            \"Detected Emotions\": emotions,\n",
    "            \"Response\": reply,\n",
    "            \"History\": self.chat_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8306fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Chatbot Pipeline Class\n",
    "\n",
    "def generate_chatbot_response(user_text, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    emotions = detect_emotions(user_text)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(\n",
    "            user_text,\n",
    "            language=language,\n",
    "            history=history if use_history else None,\n",
    "            emotions=emotions\n",
    "        )\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_text\n",
    "        inputs = t5_qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, t5_qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_text}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the chatbot pipeline.\n",
    "\n",
    "chatbot = MentalHealthChatbotPipeline(labels=EMOTION_LABELS, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95721f",
   "metadata": {},
   "source": [
    "## Gradio Interface for the Mental Health Chatbot (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0299535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fine-Tuned Models Using Combined Metadata\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path, map_location=device)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Response Generation Model & Tokenizer\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"], \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "resp_model = get_peft_model(base_model, lora_config)\n",
    "resp_model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17593d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels\n",
    "\n",
    "# Define the complete list of 28 emotion labels (as in the GoEmotions baseline)\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fine-Tuned Emotion Classifier\n",
    "\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "\n",
    "# Define a subset of emotions for routing decisions (if needed)\n",
    "\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35881b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fineâ€‘tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # Safety check: trim probabilities if there are more than expected.\n",
    "    if len(probs) > len(EMOTION_LABELS):\n",
    "        print(f\"Warning: Received {len(probs)} probabilities; expected {len(EMOTION_LABELS)}. Trimming extra values.\")\n",
    "        probs = probs[:len(EMOTION_LABELS)]\n",
    "    \n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper Functions\n",
    "\n",
    "# def transcribe_audio(audio_file):\n",
    "#     \"\"\"\n",
    "#     Converts an input audio file to text using speech recognition.\n",
    "#     Returns the transcribed text or an error message.\n",
    "#     \"\"\"\n",
    "#     recognizer = sr.Recognizer()\n",
    "#     audio = AudioSegment.from_file(audio_file)\n",
    "#     with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
    "#         audio.export(tmp.name, format=\"wav\")\n",
    "#         with sr.AudioFile(tmp.name) as source:\n",
    "#             audio_data = recognizer.record(source)\n",
    "#             try:\n",
    "#                 return recognizer.recognize_google(audio_data)\n",
    "#             except sr.UnknownValueError:\n",
    "#                 return \"[Unrecognized speech]\"\n",
    "#             except sr.RequestError:\n",
    "#                 return \"[Speech recognition failed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6981e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(\n",
    "            user_input,\n",
    "            language=language,\n",
    "            history=history if use_history else None,\n",
    "            emotions=emotions\n",
    "        )\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio Interface (Testing only)\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def chatbot_interface(user_input):\n",
    "    # 1. Detect emotion\n",
    "    emotions = predict_emotions(user_input)\n",
    "\n",
    "    # 2. Route through QA or supportive response\n",
    "    response = generate_chatbot_response(\n",
    "        user_input=user_input,\n",
    "        language=\"English\",\n",
    "        use_history=False,\n",
    "        emotions=emotions\n",
    "    )\n",
    "\n",
    "    return f\"Detected Emotions: {', '.join(emotions)}\\n\\nBot: {response}\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chatbot_interface,\n",
    "    inputs=gr.Textbox(label=\"User Input\"),\n",
    "    outputs=gr.Textbox(label=\"Chatbot Response\"),\n",
    "    title=\"ðŸ§  Happy Brain Mental Health Chatbot\",\n",
    "    description=\"Emotion-aware supportive chatbot (text only)\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Streamlit Interface\n",
    "# st.title(\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\")\n",
    "# st.write(\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\")\n",
    "\n",
    "# # Initialize conversation history\n",
    "# history = []\n",
    "\n",
    "# # Input mode\n",
    "# mode = st.radio(\"Input Mode\", [\"text\", \"voice\"])\n",
    "\n",
    "# # Language selection\n",
    "# language = st.selectbox(\"Response Language\", [\"English\", \"German\", \"Spanish\", \"French\"])\n",
    "\n",
    "# # Include chat history in response\n",
    "# use_history = st.checkbox(\"Include chat history in response\")\n",
    "\n",
    "# # Route by detected emotion\n",
    "# route_by_emotion = st.checkbox(\"Route by detected emotion\")\n",
    "\n",
    "# # Save conversation to chatlog.txt\n",
    "# persist = st.checkbox(\"Save conversation to chatlog.txt\")\n",
    "\n",
    "# # Generate chatbot response\n",
    "# if mode == \"text\":\n",
    "#     user_input = st.text_input(\"Type your message here\")\n",
    "# else:\n",
    "#     audio_input = st.file_uploader(\"Or speak here\", type=[\"wav\", \"mp3\", \"ogg\"])\n",
    "\n",
    "# if st.button(\"Generate Response\"):\n",
    "#     if mode == \"text\":\n",
    "#         response, emotions, history = generate_chatbot_response(user_input, None, mode, language, use_history, history, route_by_emotion, persist)\n",
    "#     else:\n",
    "#         response, emotions, history = generate_chatbot_response(None, audio_input, mode, language, use_history, history, route_by_emotion, persist)\n",
    "\n",
    "#     st.write(f\"Therapist Response: {response}\")\n",
    "#     st.write(f\"Detected Emotions: {emotions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf78da",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Emotion Classification\n",
    "\n",
    "def evaluate_emotion_classifier(model, tokenizer, dataset, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the emotion classifier over the provided dataset.\n",
    "    Computes micro-averaged F1, Precision, Recall, and Subset Accuracy.\n",
    "    Assumes that dataset is formatted with columns \"input_ids\", \"attention_mask\", \"labels\"\n",
    "    and that labels is a multi-label binary vector.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create a DataLoader for batch processing (if dataset is not huge, you can loop through it directly)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move inputs and labels to device\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        # Apply sigmoid for multi-label classification and threshold at 0.3\n",
    "\n",
    "        preds = (torch.sigmoid(logits) > 0.3).cpu().numpy().astype(int)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Compute micro-averaged metrics\n",
    "    \n",
    "    micro_f1 = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_precision = precision_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_recall = recall_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    subset_acc = accuracy_score(all_labels, all_preds)  # subset accuracy is strict\n",
    "    \n",
    "    return {\n",
    "        \"Emotion Classifier Micro-F1\": micro_f1,\n",
    "        \"Emotion Classifier Micro-Precision\": micro_precision,\n",
    "        \"Emotion Classifier Micro-Recall\": micro_recall,\n",
    "        \"Emotion Classifier Subset Accuracy\": subset_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05735854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"Evaluating Emotion Classifier...\")\n",
    "emo_metrics = evaluate_emotion_classifier(emo_model, emo_tokenizer, emo_test_tok)\n",
    "for metric, value in emo_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Generation Models (Response Generation and QA). We already defined a compute_resp_metrics function in the training cells.\n",
    "# Here, we define a helper to compute additional perplexity based on the evaluation loss.\n",
    "\n",
    "def evaluate_generation_model(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Uses the Seq2SeqTrainer to compute evaluation metrics over the given test dataset.\n",
    "    Adds perplexity (exp(eval_loss)) to the standard metrics.\n",
    "    \"\"\"\n",
    "    # Predict returns a dictionary with metrics: eval_loss, and any metrics computed in compute_metrics.\n",
    "\n",
    "    result = trainer.predict(test_dataset)\n",
    "    eval_loss = result.metrics.get(\"eval_loss\")\n",
    "    \n",
    "    # Compute perplexity if loss is available. (If eval_loss is zero or not available, perplexity is undefined.)\n",
    "    \n",
    "    if eval_loss is not None and eval_loss > 0:\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    else:\n",
    "        perplexity = float(\"inf\")\n",
    "    \n",
    "    # Add perplexity to the metrics dictionary.\n",
    "    result.metrics[\"perplexity\"] = perplexity\n",
    "    return result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"\\nEvaluating T5 Response Generation Model...\")\n",
    "resp_metrics = evaluate_generation_model(resp_trainer, resp_test_tok)\n",
    "for metric, value in resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating T5 QA Model...\")\n",
    "qa_metrics = evaluate_generation_model(qa_trainer, qa_test_tok)\n",
    "for metric, value in qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "def compute_generation_metrics(trainer, dataset, tokenizer):\n",
    "    rouge = load(\"rouge\")\n",
    "    bertscore = load(\"bertscore\")\n",
    "    \n",
    "    results = trainer.predict(dataset)\n",
    "    predictions, labels = results.predictions, results.label_ids\n",
    "\n",
    "    # If predictions come as a tuple (e.g., logits), extract the actual token IDs\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Convert logits to token IDs if needed (argmax across vocab dim)\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(\"\\nAdditional Generation Metrics for T5 Response Generation:\")\n",
    "additional_resp_metrics = compute_generation_metrics(resp_trainer, resp_test_tok, resp_tokenizer)\n",
    "for metric, value in additional_resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAdditional Generation Metrics for T5 QA Model:\")\n",
    "additional_qa_metrics = compute_generation_metrics(qa_trainer, qa_test_tok, t5_qa_tokenizer)\n",
    "for metric, value in additional_qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
